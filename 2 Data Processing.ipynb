{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from collections import OrderedDict\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Token\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "import pytz\n",
    "\n",
    "# Maybe this should be tested, but there are probably too few holidays to justify adding it as a feature.\n",
    "# from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "# import holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.set_start_method('fork') # Because Mac OS does not default to forking processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertFieldsintoDB(fields, table_name, cursor):\n",
    "    keys, items = zip(*fields.items())\n",
    "    keys = sql.SQL(',').join(map(sql.Identifier, keys))\n",
    "    items = sql.Literal(items)\n",
    "    table_name = sql.Identifier(table_name)\n",
    "    command = sql.SQL('INSERT INTO {} ({}) VALUES {} ON CONFLICT (id) DO NOTHING;').format(table_name, keys, items)\n",
    "    cursor.execute(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize NYT articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute('''SELECT article, id\n",
    "                        FROM articles\n",
    "                        WHERE article IS NOT NULL;''')\n",
    "    article_table = cursor.fetchall()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_blacklist = [\n",
    "    'ADP',\n",
    "    'ADV',\n",
    "    'AUX',\n",
    "    'CONJ',\n",
    "    'CCONJ',\n",
    "    'DET',\n",
    "    'INJ',\n",
    "    'PART',\n",
    "    'PRON',\n",
    "    'PUNCT',\n",
    "    'SCONJ',\n",
    "]\n",
    "\n",
    "stops = set(STOP_WORDS)\n",
    "stops.update(\n",
    "    [\"'s\", \"mr.\", \"mrs.\", \"ms.\", \"said\", \"according\"]\n",
    ")\n",
    "\n",
    "@Language.component(\"lowercase_lemmas\")\n",
    "def lowercase_lemmas(doc : spacy.tokens.doc.Doc) -> spacy.tokens.doc.Doc:\n",
    "    for token in doc:\n",
    "        token.lemma_ = token.lemma_.lower()\n",
    "    return doc\n",
    "    \n",
    "def get_is_excluded(token):\n",
    "    return (token.pos_ in POS_blacklist) or (token.lemma_ in stops)\n",
    "\n",
    "if not Token.has_extension('is_excluded'):\n",
    "    Token.set_extension('is_excluded', getter=get_is_excluded)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable = ['ner'])\n",
    "nlp.add_pipe('lowercase_lemmas', last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_generator = nlp.pipe(article_table, n_process=4, as_tuples=True)\n",
    "\n",
    "docs = []\n",
    "for doc, context in tqdm(docs_generator, total = len(article_table)):\n",
    "    docs.append((context, [token.lemma_ for token in doc if not token._.is_excluded], [sentence.text for sentence in doc.sents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''CREATE TABLE wordbags (\n",
    "\tid BIGINT PRIMARY KEY,\n",
    "    bag TEXT[],\n",
    "    sentences TEXT[]\n",
    ");''')\n",
    "\n",
    "def insertWordBagintoDB(bag, cursor):\n",
    "    fields = OrderedDict()\n",
    "    fields['id'] = bag[0]\n",
    "    fields['bag'] = bag[1]\n",
    "    fields['sentences'] = bag[2]\n",
    "    keys, items = zip(*fields.items())\n",
    "    keys = sql.SQL(',').join(map(sql.Identifier, keys))\n",
    "    items = sql.Literal(items)\n",
    "    command = sql.SQL('INSERT INTO wordbags ({}) VALUES {} ON CONFLICT (id) DO NOTHING;').format(keys, items)\n",
    "    cursor.execute(command)\n",
    "\n",
    "for bag in docs:\n",
    "    insertWordBagintoDB(bag, cursor)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Train-Validation-Test Split\n",
    "\n",
    "I might add new articles to the dataset in the future, so I need a robust and consistent way to train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zlib import crc32\n",
    "\n",
    "def twitter_id_hash(id : int) -> int:\n",
    "    return crc32((id & 0xFFFFFFFF).to_bytes(4, byteorder = 'big'))\n",
    "\n",
    "def train_or_test(hash_ : int) -> str:\n",
    "    TWO_POW_32 = 4294967296\n",
    "    if hash_ < 0.6 * TWO_POW_32:\n",
    "        return 'train'\n",
    "    elif hash_ < 0.8 * TWO_POW_32:\n",
    "        return 'valid'\n",
    "    else:\n",
    "        return 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''SELECT id\n",
    "                    FROM tweets;''')\n",
    "ids = [item[0] for item in cursor.fetchall()]\n",
    "\n",
    "cursor.execute('''CREATE TABLE traintest (\n",
    "\tid BIGINT PRIMARY KEY,\n",
    "    hash BIGINT,\n",
    "    split VARCHAR(5)\n",
    ");''')\n",
    "\n",
    "def insertTrainTestintoDB(id, cursor):\n",
    "    fields = OrderedDict()\n",
    "    fields['id'] = id\n",
    "    fields['hash'] = twitter_id_hash(id)\n",
    "    fields['split'] = train_or_test(fields['hash'])\n",
    "    keys, items = zip(*fields.items())\n",
    "    keys = sql.SQL(',').join(map(sql.Identifier, keys))\n",
    "    items = sql.Literal(items)\n",
    "    command = sql.SQL('INSERT INTO traintest ({}) VALUES {} ON CONFLICT (id) DO NOTHING;').format(keys, items)\n",
    "    cursor.execute(command)\n",
    "\n",
    "for id in ids:\n",
    "    insertTrainTestintoDB(id, cursor)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''SELECT tweets.id, tweets.text, articles.title, articles.summary, articles.article\n",
    "                    FROM tweets\n",
    "                    LEFT JOIN articles\n",
    "                    ON tweets.id = articles.id;''')\n",
    "texts = cursor.fetchall()\n",
    "\n",
    "cursor.execute('''CREATE TABLE textlengths (\n",
    "\tid BIGINT PRIMARY KEY,\n",
    "    tweetlength INT,\n",
    "    titlelength INT,\n",
    "    summarylength INT,\n",
    "    articlelength INT\n",
    ");''')\n",
    "\n",
    "def text_len(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    return len(text.strip().split())\n",
    "\n",
    "def insertLengthsintoDB(row, cursor):\n",
    "    fields = OrderedDict()\n",
    "    fields['id'] = row[0]\n",
    "    for idx, fieldname in enumerate(['tweet', 'title', 'summary', 'article']):\n",
    "        fields[fieldname + 'length'] = text_len(row[idx + 1])\n",
    "    insertFieldsintoDB(fields, 'textlengths', cursor)\n",
    "\n",
    "for row in texts:\n",
    "    insertLengthsintoDB(row, cursor)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute('''SELECT id, url \n",
    "                        FROM articles;''')\n",
    "    urls = cursor.fetchall()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r'^https://(?:www|cooking)\\.nytimes\\.com/.*?/?(\\D*)/(?:\\d*/)?[^/]*$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_process(text):\n",
    "    if 'opinion' in text:\n",
    "        return 'opinion'\n",
    "    if 'science' in text:\n",
    "        return 'science'\n",
    "    if 'fashion' in text:\n",
    "        return 'fashion'\n",
    "    if 'technology' in text:\n",
    "        return 'technology'\n",
    "    # Should 'well' also be consolidated?\n",
    "    text = text.replace('video/', '')\n",
    "    text = text.replace('sports/ncaa', 'sports/')\n",
    "    text = text.replace('us/elections', 'us/politics')\n",
    "    return text\n",
    "\n",
    "def url_match(url):\n",
    "    if 'wirecutter' in url:\n",
    "        return 'wirecutter'\n",
    "    m = regex.match(url)\n",
    "    if m is not None:\n",
    "        return section_process(m.group(1))\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''CREATE TABLE sections (\n",
    "\tid BIGINT PRIMARY KEY,\n",
    "    section VARCHAR(255)\n",
    ");''')\n",
    "\n",
    "def insertSectionintoDB(id, url, cursor):\n",
    "    fields = OrderedDict()\n",
    "    fields['id'] = id\n",
    "    fields['section'] = url_match(url)\n",
    "    insertFieldsintoDB(fields, 'sections', cursor)\n",
    "\n",
    "for id, url in urls:\n",
    "    insertSectionintoDB(id, url, cursor)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet date & time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute('''SELECT id, date \n",
    "                        FROM tweets;''')\n",
    "    dates = cursor.fetchall()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should time be encoded?  Here are some ways of doing it:\n",
    "- Divide the day into sections, and then one-hot encode the sections. Dividing the day into hours is an obvious choice, but there are many other ways to divide the day (e.g. morning, afternoon, and night). The division of the day doesn't even need to be a partition - a timestamp could belong to multiple sections.\n",
    "    - Pros:\n",
    "        - Easy to do\n",
    "    - Cons:\n",
    "        - Discrete\n",
    "        - Ignores the cyclic nature of time\n",
    "- Divide the day into sections, and integer encode the sections.\n",
    "    - Pros:\n",
    "        - Easy to do\n",
    "        - Unlike one-hot encoding, only increases the dimensionality of the features by 1\n",
    "        - Some (but not all) implementations of decision-tree-based algorithms know how to handle integer-encoded categorical variables\n",
    "    - Cons:\n",
    "        - Discrete\n",
    "        - Ignores the cyclic nature of time\n",
    "        - Imposes false ordering and arithmetic relationships between the sections\n",
    "- Transform the time with sine and cosine.\n",
    "    - Pros:\n",
    "        - Continuous\n",
    "        - Incorporates the cyclic nature of time (e.g. if partitioning by hour, 23:59 is closer to 00:01 than to 23:00)\n",
    "    - Cons:\n",
    "        - Tree-based algorithms split on a single feature, but the time is encoded by two features\n",
    "- Transform the time with radial basis functions with periodic boundary conditions (i.e. the basis functions are periodic modulo one day).\n",
    "    - Pros:\n",
    "        - Continuous\n",
    "        - Incorporates the cyclic nature of time\n",
    "    - Cons:\n",
    "        - Need to decide the number of basis functions, their widths, and their locations. This means more hyperparameters to tune...\n",
    "\n",
    "For now, I will just precompute the numbers of seconds since Eastern-time midnight and store this data into an SQL database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "seconds = []\n",
    "months = []\n",
    "dayofweek = []\n",
    "for id, dt in dates:\n",
    "    ids.append(id)\n",
    "    dt_eastern = dt.astimezone(pytz.timezone('US/Eastern')) # Ensure it is Eastern time\n",
    "    secs = (dt_eastern - dt_eastern.replace(hour = 0, minute = 0, second = 0, microsecond = 0)).total_seconds()\n",
    "    seconds.append(secs)\n",
    "    months.append(dt_eastern.month)\n",
    "    dayofweek.append(dt_eastern.weekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''CREATE TABLE timeinfo (\n",
    "\tid BIGINT PRIMARY KEY,\n",
    "    seconds INT,\n",
    "    month INT,\n",
    "    dayofweek INT\n",
    ");''')\n",
    "\n",
    "def insertTimeintoDB(id, sec, mon, dow, cursor):\n",
    "    fields = OrderedDict()\n",
    "    fields['id'] = id\n",
    "    fields['seconds'] = sec\n",
    "    fields['month'] = mon\n",
    "    fields['dayofweek'] = dow\n",
    "    insertFieldsintoDB(fields, 'timeinfo', cursor)\n",
    "\n",
    "for args in zip(ids, seconds, months, dayofweek):\n",
    "    insertTimeintoDB(*args, cursor)\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52dc165078b550922a92e20afc6187e50fa6838252eb052b89f3983eda27ca00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
