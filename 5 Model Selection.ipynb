{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "from typing import List, Tuple\n",
    "import logging\n",
    "from itertools import repeat, chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tables\n",
    "\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Get the training and validation datasets from an SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query = sql.SQL(\"\"\"SELECT ttv.id, tweets.retweet_count, tweets.like_count, tweets.reply_count,\n",
    "    \n",
    "                        tweets.video AS tweet_has_video,\n",
    "                        tweets.photo AS tweet_has_photo,\n",
    "\n",
    "                        articles.video AS article_has_video,\n",
    "                        articles.audio AS article_has_audio,\n",
    "                        articles.comments,\n",
    "\n",
    "                        textlengths.tweetlength,\n",
    "                        textlengths.titlelength,\n",
    "                        textlengths.summarylength,\n",
    "                        textlengths.articlelength,\n",
    "\n",
    "                        sections.section,\n",
    "\n",
    "                        timeinfo.seconds,\n",
    "                        timeinfo.month,\n",
    "                        timeinfo.dayofweek,\n",
    "                        \n",
    "                        sentiment.vader_tweet_texts_neg,\n",
    "                        sentiment.vader_tweet_texts_neu,\n",
    "                        sentiment.vader_tweet_texts_pos,\n",
    "                        sentiment.vader_tweet_texts_compound,\n",
    "                        sentiment.vader_article_titles_neg,\n",
    "                        sentiment.vader_article_titles_neu,\n",
    "                        sentiment.vader_article_titles_pos,\n",
    "                        sentiment.vader_article_titles_compound,\n",
    "                        sentiment.vader_article_summaries_neg,\n",
    "                        sentiment.vader_article_summaries_neu,\n",
    "                        sentiment.vader_article_summaries_pos,\n",
    "                        sentiment.vader_article_summaries_compound,\n",
    "                        sentiment.vader_article_main_neg,\n",
    "                        sentiment.vader_article_main_neu,\n",
    "                        sentiment.vader_article_main_pos,\n",
    "                        sentiment.vader_article_main_compound,\n",
    "\n",
    "                        sentiment.distilbert_tweet_texts_negative,\n",
    "                        sentiment.distilbert_tweet_texts_positive,\n",
    "                        sentiment.distilbert_article_titles_negative,\n",
    "                        sentiment.distilbert_article_titles_positive,\n",
    "                        sentiment.distilbert_article_summaries_negative,\n",
    "                        sentiment.distilbert_article_summaries_positive,\n",
    "                        sentiment.distilbert_article_main_negative,\n",
    "                        sentiment.distilbert_article_main_positive,\n",
    "\n",
    "                        sentiment.roberta_tweet_texts_negative,\n",
    "                        sentiment.roberta_tweet_texts_positive,\n",
    "                        sentiment.roberta_tweet_texts_neutral,\n",
    "                        sentiment.roberta_article_titles_negative,\n",
    "                        sentiment.roberta_article_titles_positive,\n",
    "                        sentiment.roberta_article_titles_neutral,\n",
    "                        sentiment.roberta_article_summaries_negative,\n",
    "                        sentiment.roberta_article_summaries_positive,\n",
    "                        sentiment.roberta_article_summaries_neutral,\n",
    "                        sentiment.roberta_article_main_negative,\n",
    "                        sentiment.roberta_article_main_positive,\n",
    "                        sentiment.roberta_article_main_neutral,\n",
    "\n",
    "                        sentiment.siebert_tweet_texts_negative,\n",
    "                        sentiment.siebert_tweet_texts_positive,\n",
    "                        sentiment.siebert_article_titles_negative,\n",
    "                        sentiment.siebert_article_titles_positive,\n",
    "                        sentiment.siebert_article_summaries_negative,\n",
    "                        sentiment.siebert_article_summaries_positive,\n",
    "                        sentiment.siebert_article_main_negative,\n",
    "                        sentiment.siebert_article_main_positive\n",
    "\n",
    "                    FROM (SELECT id FROM traintest WHERE split = {}) AS ttv\n",
    "                    INNER JOIN tweets ON ttv.id = tweets.id\n",
    "                    INNER JOIN articles ON ttv.id = articles.id\n",
    "                    INNER JOIN textlengths ON ttv.id = textlengths.id\n",
    "                    INNER JOIN sections ON ttv.id = sections.id\n",
    "                    INNER JOIN timeinfo ON ttv.id = timeinfo.id\n",
    "                    INNER JOIN sentiment ON ttv.id = sentiment.id\n",
    "                    WHERE tweets.date < {}\n",
    "                    ;\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_DATE = '2022-04-16'\n",
    "\n",
    "with psycopg2.connect(host = 'localhost', database = 'nytpopular') as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(db_query.format(sql.Literal('train'), sql.Literal(CUTOFF_DATE)))\n",
    "        train = cursor.fetchall()\n",
    "        train_column_names = [description[0] for description in cursor.description]\n",
    "        cursor.execute(db_query.format(sql.Literal('valid'), sql.Literal(CUTOFF_DATE)))\n",
    "        val = cursor.fetchall()\n",
    "        val_column_names = [description[0] for description in cursor.description]\n",
    "        cursor.execute(db_query.format(sql.Literal('test'), sql.Literal(CUTOFF_DATE)))\n",
    "        test = cursor.fetchall()\n",
    "        test_column_names = [description[0] for description in cursor.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_target(target : pd.Series) -> pd.Series:\n",
    "    return np.log10(target + 1.0)\n",
    "\n",
    "def preprocess(dataset : List[Tuple], columns : List[str]) -> pd.DataFrame:\n",
    "    X = pd.DataFrame(dataset, columns = columns).set_index('id')\n",
    "    likes = transform_target(X['like_count'])\n",
    "    retweets = transform_target(X['retweet_count'])\n",
    "    replies = transform_target(X['reply_count'])\n",
    "    X = X.drop(['like_count', 'retweet_count', 'reply_count'], axis = 1)\n",
    "    for key in ['tweet_has_video', 'tweet_has_photo', 'article_has_video', 'article_has_audio']: # 0/1 encode these bools\n",
    "        X[key] = X[key].astype(int)\n",
    "    # Were comments enabled? The number of comments cannot be kept as a feature because this is not information that is available before an article is published.\n",
    "    # It's possible that \n",
    "    X['comments'] = (X['comments']/X['comments']).fillna(0).astype(int)\n",
    "    return X, likes, retweets, replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_sections(X): # Danger: intentionally mutates the input!!!\n",
    "    section_counts = X['section'].value_counts()\n",
    "    MIN_COUNT = 2\n",
    "    sections_to_be_removed = section_counts[section_counts <= MIN_COUNT].index\n",
    "    X.loc[X['section'].isin(sections_to_be_removed), 'section'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, likes_train, retweets_train, replies_train = preprocess(train, train_column_names)\n",
    "X_val, likes_val, retweets_val, replies_val = preprocess(val, val_column_names)\n",
    "X_test, likes_test, retweets_test, replies_test = preprocess(test, test_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rare_sections(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bseline Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_val_combined = pd.concat([X_train, X_val])\n",
    "X_combined, likes_combined, retweets_combined, replies_combined = preprocess([*train, *val], train_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy in ['mean', 'median']:\n",
    "    for label, y_combined, y_test in zip(['likes', 'retweets', 'replies'], [likes_combined, retweets_combined, replies_combined], [likes_test, retweets_test, replies_test]):\n",
    "        baseline = DummyRegressor(strategy=strategy)\n",
    "        baseline.fit(X_combined, y_combined)\n",
    "        print(f'R^2 for the {strategy} model for {label} is {baseline.score(X_test, y_test)}.') # Should be near 0, since R^2 compares the model to the mean model (i.e. exactly zero for the training data).\n",
    "    print()\n",
    "\n",
    "# A little surprising to me that the median model is actually slightly worse than the mean model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "Pipeline transformers for selecting features and getting the data into the correct format.\n",
    "\n",
    "I will use LightGBM for tree-based gradient boosting because LightGBM is accurate, fast, handles categorical variables without needed to one-hot encode, and handles missing data (although how missing data is handled is [not always intelligent](https://github.com/microsoft/LightGBM/issues/2921).\n",
    "\n",
    "Things to watch out for:\n",
    "- Although LightGBM can handle categorical features in pandas DataFrames, this is potentially dangerous if new unseen categories appear after training. I will use scikit-learn to encode categorical data into integers.\n",
    "- Although multicollinearity will not significantly adversely affect predictions, it may make interpretation difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegerCategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        # encoded_missing_value is new in scikit-learn 1.1. Remove it for older versions.\n",
    "        self._ordinalencoder = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = np.nan, encoded_missing_value = np.nan)\n",
    "        self._ordinalencoder.fit(X[[self.column_name]])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.column_name] = self._ordinalencoder.transform(X[[self.column_name]])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAILY_SECONDS = 86400\n",
    "HOURLY_SECONDS = 3600\n",
    "\n",
    "def gaussian(x, center, width):\n",
    "    return np.exp(-(x - center)**2 / (2 * width **2))\n",
    "\n",
    "def periodic_gaussian(x, center_in_hours, width_in_hours): # Not actually periodic\n",
    "    center = center_in_hours * HOURLY_SECONDS\n",
    "    width = width_in_hours * HOURLY_SECONDS\n",
    "    return gaussian(x - DAILY_SECONDS, center, width) + gaussian(x, center, width) + gaussian(x + DAILY_SECONDS, center, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, time_encode_type, n_time_gaussians = None):\n",
    "        if time_encode_type == 'raw':\n",
    "            assert n_time_gaussians is None, 'n_time_gaussians specified when time_encode_type == \"raw\"'\n",
    "        elif time_encode_type == 'rbf': # Radial Basis Functions\n",
    "            assert n_time_gaussians in [4, 6, 12, 24], 'n_time_gaussians must divide 24'\n",
    "            self.n_time_gaussians = n_time_gaussians\n",
    "        else:\n",
    "            assert False, 'Unknown time_encode_type'\n",
    "        self.time_encode_type = time_encode_type\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.time_encode_type == 'rbf':\n",
    "            seconds = X['seconds']\n",
    "            X = X.drop(['seconds'], axis = 1)\n",
    "            for basis_index in range(self.n_time_gaussians):\n",
    "                hour = basis_index * 24 // self.n_time_gaussians\n",
    "                X[f'hour_{hour}'] = periodic_gaussian(seconds, hour, 24 // self.n_time_gaussians) # A measure of how far away the time is from the \"hour\". ~1 represents right on the hour, ~0 represents far away from the hour.\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, sentiment_type):\n",
    "        self._sentiment_models = {'vader', 'distilbert', 'roberta', 'siebert'}\n",
    "        assert sentiment_type in self._sentiment_models, 'Unknown sentiment analyzer'\n",
    "        self.sentiment_type = sentiment_type\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        unwanted_columns = [column for column in X.columns if any(sentiment_type in column for sentiment_type in self._sentiment_models.difference({self.sentiment_type}))]\n",
    "        X = X.drop(unwanted_columns, axis = 1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_valid_nTopics = set(chain(range(5,30,5),range(30, 200 + 1, 10)))\n",
    "\n",
    "def load_topic_df(filename : str, nTopics : int, top_topics : bool = False, include_scores = False, MAX_TOPICS = 5) -> pd.DataFrame:\n",
    "    with tables.open_file(filename, mode = 'r') as f:\n",
    "        table = f.root.topic[f'ntopics{nTopics}']\n",
    "        ids, topicvectors = zip(*[(x['id'], x['topicvector']) for x in table.iterrows()])\n",
    "        if top_topics:\n",
    "            topic_ranks = [(-tv).argsort()[:MAX_TOPICS] for tv in topicvectors]\n",
    "            if include_scores:\n",
    "                # sorted_topic_scores = [np.sort(tv)[::-1] for tv in topicvectors]\n",
    "                sorted_topic_scores = [[*tr, *tv[tr]] for tr, tv in zip(topic_ranks, topicvectors)]\n",
    "                return pd.DataFrame(sorted_topic_scores, index = ids, columns = [f'best_topic_{n:03}' for n in range(MAX_TOPICS)] + [f'best_topic_scores_{n:03}' for n in range(MAX_TOPICS)])\n",
    "            else:\n",
    "                return pd.DataFrame(topic_ranks, index = ids, columns = [f'best_topic_{n:03}' for n in range(MAX_TOPICS)])\n",
    "        else:\n",
    "            return pd.DataFrame(topicvectors, index = ids, columns = [f'topic_{n:03}' for n in range(nTopics)])\n",
    "\n",
    "class TopicLoader(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, nTopics : int, top_5 : bool = False, include_scores : bool = False, symmetricbeta : bool = False):\n",
    "        assert nTopics in _valid_nTopics, 'Invalid number of topics'\n",
    "        assert (not include_scores) or top_5, 'include_scores provided when top_5 is False'\n",
    "        self.nTopics = nTopics\n",
    "        self.top_5 = top_5\n",
    "        self.include_scores = include_scores\n",
    "        self.symmetricbeta = symmetricbeta\n",
    "        if symmetricbeta:\n",
    "            filename = r'GensimModels/article_data_symmetricbeta.h5'\n",
    "        else:\n",
    "            filename = r'GensimModels/article_data.h5'\n",
    "        self.topics_df = load_topic_df(filename, nTopics, top_5, include_scores)\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.merge(X, self.topics_df, how = 'inner', left_index = True, right_index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52dc165078b550922a92e20afc6187e50fa6838252eb052b89f3983eda27ca00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
