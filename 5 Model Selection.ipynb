{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "from typing import List, Tuple\n",
    "import logging\n",
    "from itertools import repeat, chain\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tables\n",
    "\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import lightgbm as lgbm\n",
    "\n",
    "# from ray import tune\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Get the training and validation datasets from an SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query = sql.SQL(\"\"\"SELECT ttv.id, tweets.retweet_count, tweets.like_count, tweets.reply_count,\n",
    "    \n",
    "                        tweets.video AS tweet_has_video,\n",
    "                        tweets.photo AS tweet_has_photo,\n",
    "\n",
    "                        articles.video AS article_has_video,\n",
    "                        articles.audio AS article_has_audio,\n",
    "                        articles.comments,\n",
    "\n",
    "                        textlengths.tweetlength,\n",
    "                        textlengths.titlelength,\n",
    "                        textlengths.summarylength,\n",
    "                        textlengths.articlelength,\n",
    "\n",
    "                        sections.section,\n",
    "\n",
    "                        timeinfo.seconds,\n",
    "                        timeinfo.month,\n",
    "                        timeinfo.dayofweek,\n",
    "                        \n",
    "                        sentiment.vader_tweet_texts_neg,\n",
    "                        sentiment.vader_tweet_texts_neu,\n",
    "                        sentiment.vader_tweet_texts_pos,\n",
    "                        sentiment.vader_tweet_texts_compound,\n",
    "                        sentiment.vader_article_titles_neg,\n",
    "                        sentiment.vader_article_titles_neu,\n",
    "                        sentiment.vader_article_titles_pos,\n",
    "                        sentiment.vader_article_titles_compound,\n",
    "                        sentiment.vader_article_summaries_neg,\n",
    "                        sentiment.vader_article_summaries_neu,\n",
    "                        sentiment.vader_article_summaries_pos,\n",
    "                        sentiment.vader_article_summaries_compound,\n",
    "                        sentiment.vader_article_main_neg,\n",
    "                        sentiment.vader_article_main_neu,\n",
    "                        sentiment.vader_article_main_pos,\n",
    "                        sentiment.vader_article_main_compound,\n",
    "\n",
    "                        sentiment.distilbert_tweet_texts_negative,\n",
    "                        sentiment.distilbert_tweet_texts_positive,\n",
    "                        sentiment.distilbert_article_titles_negative,\n",
    "                        sentiment.distilbert_article_titles_positive,\n",
    "                        sentiment.distilbert_article_summaries_negative,\n",
    "                        sentiment.distilbert_article_summaries_positive,\n",
    "                        sentiment.distilbert_article_main_negative,\n",
    "                        sentiment.distilbert_article_main_positive,\n",
    "\n",
    "                        sentiment.roberta_tweet_texts_negative,\n",
    "                        sentiment.roberta_tweet_texts_positive,\n",
    "                        sentiment.roberta_tweet_texts_neutral,\n",
    "                        sentiment.roberta_article_titles_negative,\n",
    "                        sentiment.roberta_article_titles_positive,\n",
    "                        sentiment.roberta_article_titles_neutral,\n",
    "                        sentiment.roberta_article_summaries_negative,\n",
    "                        sentiment.roberta_article_summaries_positive,\n",
    "                        sentiment.roberta_article_summaries_neutral,\n",
    "                        sentiment.roberta_article_main_negative,\n",
    "                        sentiment.roberta_article_main_positive,\n",
    "                        sentiment.roberta_article_main_neutral,\n",
    "\n",
    "                        sentiment.siebert_tweet_texts_negative,\n",
    "                        sentiment.siebert_tweet_texts_positive,\n",
    "                        sentiment.siebert_article_titles_negative,\n",
    "                        sentiment.siebert_article_titles_positive,\n",
    "                        sentiment.siebert_article_summaries_negative,\n",
    "                        sentiment.siebert_article_summaries_positive,\n",
    "                        sentiment.siebert_article_main_negative,\n",
    "                        sentiment.siebert_article_main_positive\n",
    "\n",
    "                    FROM (SELECT id FROM traintest WHERE split = {}) AS ttv\n",
    "                    INNER JOIN tweets ON ttv.id = tweets.id\n",
    "                    INNER JOIN articles ON ttv.id = articles.id\n",
    "                    INNER JOIN textlengths ON ttv.id = textlengths.id\n",
    "                    INNER JOIN sections ON ttv.id = sections.id\n",
    "                    INNER JOIN timeinfo ON ttv.id = timeinfo.id\n",
    "                    INNER JOIN sentiment ON ttv.id = sentiment.id\n",
    "                    WHERE tweets.date < {}\n",
    "                    ;\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_DATE = '2022-04-16'\n",
    "\n",
    "with psycopg2.connect(host = 'localhost', database = 'nytpopular') as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(db_query.format(sql.Literal('train'), sql.Literal(CUTOFF_DATE)))\n",
    "        train = cursor.fetchall()\n",
    "        train_column_names = [description[0] for description in cursor.description]\n",
    "        cursor.execute(db_query.format(sql.Literal('valid'), sql.Literal(CUTOFF_DATE)))\n",
    "        val = cursor.fetchall()\n",
    "        val_column_names = [description[0] for description in cursor.description]\n",
    "        cursor.execute(db_query.format(sql.Literal('test'), sql.Literal(CUTOFF_DATE)))\n",
    "        test = cursor.fetchall()\n",
    "        test_column_names = [description[0] for description in cursor.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_target(target : pd.Series) -> pd.Series:\n",
    "    return np.log10(target + 1.0)\n",
    "\n",
    "def preprocess(dataset : List[Tuple], columns : List[str]) -> pd.DataFrame:\n",
    "    X = pd.DataFrame(dataset, columns = columns).set_index('id')\n",
    "    likes = transform_target(X['like_count'])\n",
    "    retweets = transform_target(X['retweet_count'])\n",
    "    replies = transform_target(X['reply_count'])\n",
    "    X = X.drop(['like_count', 'retweet_count', 'reply_count'], axis = 1)\n",
    "    for key in ['tweet_has_video', 'tweet_has_photo', 'article_has_video', 'article_has_audio']: # 0/1 encode these bools\n",
    "        X[key] = X[key].astype(int)\n",
    "    # Were comments enabled? The number of comments cannot be kept as a feature because this is not information that is available before an article is published.\n",
    "    # It's possible that \n",
    "    X['comments'] = (X['comments']/X['comments']).fillna(0).astype(int)\n",
    "    return X, likes, retweets, replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_sections(X): # Danger: intentionally mutates the input!!!\n",
    "    section_counts = X['section'].value_counts()\n",
    "    MIN_COUNT = 2\n",
    "    sections_to_be_removed = section_counts[section_counts <= MIN_COUNT].index\n",
    "    X.loc[X['section'].isin(sections_to_be_removed), 'section'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, likes_train, retweets_train, replies_train = preprocess(train, train_column_names)\n",
    "X_val, likes_val, retweets_val, replies_val = preprocess(val, val_column_names)\n",
    "X_test, likes_test, retweets_test, replies_test = preprocess(test, test_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rare_sections(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_val_combined = pd.concat([X_train, X_val])\n",
    "X_combined, likes_combined, retweets_combined, replies_combined = preprocess([*train, *val], train_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy in ['mean', 'median']:\n",
    "    for label, y_combined, y_test in zip(['likes', 'retweets', 'replies'], [likes_combined, retweets_combined, replies_combined], [likes_test, retweets_test, replies_test]):\n",
    "        baseline = DummyRegressor(strategy=strategy)\n",
    "        baseline.fit(X_combined, y_combined)\n",
    "        print(f'R^2 for the {strategy} model for {label} is {baseline.score(X_test, y_test)}.') # Should be near 0, since R^2 compares the model to the mean model (i.e. exactly zero for the training data).\n",
    "    print()\n",
    "\n",
    "# A little surprising to me that the median model is actually slightly worse than the mean model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "Pipeline transformers for selecting features and getting the data into the correct format.\n",
    "\n",
    "I will use LightGBM for tree-based gradient boosting because LightGBM is accurate, fast, handles categorical variables without needed to one-hot encode, and handles missing data (although how missing data is handled is [not always intelligent](https://github.com/microsoft/LightGBM/issues/2921).\n",
    "\n",
    "Things to watch out for:\n",
    "- Although LightGBM can handle categorical features in pandas DataFrames, this is potentially dangerous if new unseen categories appear after training. I will use scikit-learn to encode categorical data into integers.\n",
    "- Although multicollinearity will not significantly adversely affect predictions, it may make interpretation difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegerCategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, column_name):\n",
    "        self.column_name = column_name\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        # encoded_missing_value is new in scikit-learn 1.1. Remove it for older versions.\n",
    "        self._ordinalencoder = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = np.nan, encoded_missing_value = np.nan)\n",
    "        self._ordinalencoder.fit(X[[self.column_name]])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X[self.column_name] = self._ordinalencoder.transform(X[[self.column_name]])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAILY_SECONDS = 86400\n",
    "HOURLY_SECONDS = 3600\n",
    "\n",
    "def gaussian(x, center, width):\n",
    "    return np.exp(-(x - center)**2 / (2 * width **2))\n",
    "\n",
    "def periodic_gaussian(x, center_in_hours, width_in_hours): # Not actually periodic\n",
    "    center = center_in_hours * HOURLY_SECONDS\n",
    "    width = width_in_hours * HOURLY_SECONDS\n",
    "    return gaussian(x - DAILY_SECONDS, center, width) + gaussian(x, center, width) + gaussian(x + DAILY_SECONDS, center, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, time_encode_type : str, n_time_gaussians = None):\n",
    "        if time_encode_type == 'raw':\n",
    "            assert n_time_gaussians is None, 'n_time_gaussians specified when time_encode_type == \"raw\"'\n",
    "        elif time_encode_type == 'rbf': # Radial Basis Functions\n",
    "            assert n_time_gaussians in [4, 6, 12, 24], 'n_time_gaussians must divide 24'\n",
    "            self.n_time_gaussians = n_time_gaussians\n",
    "        else:\n",
    "            assert False, 'Unknown time_encode_type'\n",
    "        self.time_encode_type = time_encode_type\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.time_encode_type == 'rbf':\n",
    "            seconds = X['seconds']\n",
    "            X = X.drop(['seconds'], axis = 1)\n",
    "            for basis_index in range(self.n_time_gaussians):\n",
    "                hour = basis_index * 24 // self.n_time_gaussians\n",
    "                X[f'hour_{hour}'] = periodic_gaussian(seconds, hour, 24 // self.n_time_gaussians) # A measure of how far away the time is from the \"hour\". ~1 represents right on the hour, ~0 represents far away from the hour.\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, sentiment_type):\n",
    "        self._sentiment_models = {'none', 'vader', 'distilbert', 'roberta', 'siebert'}\n",
    "        assert sentiment_type in self._sentiment_models, 'Unknown sentiment analyzer'\n",
    "        self.sentiment_type = sentiment_type\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        unwanted_columns = [column for column in X.columns if any(sentiment_type in column for sentiment_type in self._sentiment_models.difference({self.sentiment_type}))]\n",
    "        X = X.drop(unwanted_columns, axis = 1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnRemover(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, drop_month = False, drop_dayofweek = False, drop_comments = False, drop_lengths = False):\n",
    "        self.drop_month = drop_month\n",
    "        self.drop_dayofweek = drop_dayofweek\n",
    "        self.drop_comments = drop_comments\n",
    "        self.drop_lengths = drop_lengths\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        unwanted_columns = []\n",
    "        if self.drop_month:\n",
    "            unwanted_columns.append('month')\n",
    "        if self.drop_dayofweek:\n",
    "            unwanted_columns.append('dayofweek')\n",
    "        if self.drop_comments:\n",
    "            unwanted_columns.append('comments')\n",
    "        if self.drop_lengths:\n",
    "            unwanted_columns.extend([column for column in X.columns if 'length' in column])\n",
    "        X = X.drop(unwanted_columns, axis = 1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_nTopics_list = list(chain(range(5,30,5),range(30, 200 + 1, 10)))\n",
    "_valid_nTopics = set(_nTopics_list)\n",
    "\n",
    "def load_topic_df(filename : str, nTopics : int, top_topics : bool = False, include_scores = False, MAX_TOPICS = 5) -> pd.DataFrame:\n",
    "    with tables.open_file(filename, mode = 'r') as f:\n",
    "        table = f.root.topic[f'ntopics{nTopics}']\n",
    "        ids, topicvectors = zip(*[(x['id'], x['topicvector']) for x in table.iterrows()])\n",
    "        if top_topics:\n",
    "            topic_ranks = [(-tv).argsort()[:MAX_TOPICS] for tv in topicvectors]\n",
    "            if include_scores:\n",
    "                # sorted_topic_scores = [np.sort(tv)[::-1] for tv in topicvectors]\n",
    "                sorted_topic_scores = [[*tr, *tv[tr]] for tr, tv in zip(topic_ranks, topicvectors)]\n",
    "                return pd.DataFrame(sorted_topic_scores, index = ids, columns = [f'best_topic_{n:03}' for n in range(MAX_TOPICS)] + [f'best_topic_scores_{n:03}' for n in range(MAX_TOPICS)])\n",
    "            else:\n",
    "                return pd.DataFrame(topic_ranks, index = ids, columns = [f'best_topic_{n:03}' for n in range(MAX_TOPICS)])\n",
    "        else:\n",
    "            return pd.DataFrame(topicvectors, index = ids, columns = [f'topic_{n:03}' for n in range(nTopics)])\n",
    "\n",
    "class TopicLoader(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, nTopics : int, top_5 : bool = False, include_scores : bool = False, symmetricbeta : bool = False):\n",
    "        assert nTopics in _valid_nTopics, 'Invalid number of topics'\n",
    "        assert (not include_scores) or top_5, 'include_scores provided when top_5 is False'\n",
    "        self.nTopics = nTopics\n",
    "        self.top_5 = top_5\n",
    "        self.include_scores = include_scores\n",
    "        self.symmetricbeta = symmetricbeta\n",
    "        if symmetricbeta:\n",
    "            filename = r'GensimModels/article_data_symmetricbeta.h5'\n",
    "        else:\n",
    "            filename = r'GensimModels/article_data.h5'\n",
    "        self.topics_df = load_topic_df(filename, nTopics, top_5, include_scores)\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return pd.merge(X, self.topics_df, how = 'inner', left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_topic_regex = re.compile(r'^best_topic_\\d{3}$')\n",
    "\n",
    "def categorical_identifier(label : str) -> bool:\n",
    "    if label in {'section', 'month', 'dayofweek', 'comments', 'article_has_audio', 'article_has_video', 'tweet_has_video', 'tweet_has_photo'}:\n",
    "        return True\n",
    "    elif best_topic_regex.match(label):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Fix code duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_logger = logging.getLogger('lgbm')\n",
    "loghandle = logging.FileHandler(f'TreeModels/logs/lgbm.log')\n",
    "logformat = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "loghandle.setFormatter(logformat)\n",
    "lgbm_logger.addHandler(loghandle)\n",
    "lgbm_logger.setLevel(logging.INFO)\n",
    "\n",
    "lgbm.register_logger(lgbm_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_factory(X_train, y_train, X_val, y_val, evaluate = False, test_set = None, n_estimators = None):\n",
    "\n",
    "    def objective(trial : optuna.Trial):\n",
    "\n",
    "        TIME_ENCODE_TYPE = trial.suggest_categorical('time_encode_type', ['raw', 'rbf'])\n",
    "        if TIME_ENCODE_TYPE == 'rbf':\n",
    "            N_TIME_GAUSSIANS = [4, 6, 12, 24][trial.suggest_int('n_time_gaussians_INDEX', 0, 3)]\n",
    "        else:\n",
    "            N_TIME_GAUSSIANS = None\n",
    "        SENTIMENT_TYPE = trial.suggest_categorical('sentiment_type', ['vader', 'distilbert', 'roberta', 'siebert'])\n",
    "        NTOPICS = _nTopics_list[trial.suggest_int('nTopics_INDEX', 0, len(_nTopics_list) - 1)]\n",
    "        TOP_5 = trial.suggest_categorical('top_5', [True, False])\n",
    "        if TOP_5:\n",
    "            INCLUDE_SCORES = trial.suggest_categorical('include_scores', [True, False])\n",
    "        else:\n",
    "            INCLUDE_SCORES = None\n",
    "        SYMMETRICBETA = trial.suggest_categorical('symmetricbeta', [True, False])\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            ('section_encoder', IntegerCategoricalEncoder('section')),\n",
    "            ('time_encoder', TimeEncoder(time_encode_type = TIME_ENCODE_TYPE, n_time_gaussians = N_TIME_GAUSSIANS)),\n",
    "            ('sentiment_selector', SentimentSelector(sentiment_type = SENTIMENT_TYPE)),\n",
    "            ('topic_loader', TopicLoader(nTopics = NTOPICS, top_5 = TOP_5, include_scores = INCLUDE_SCORES, symmetricbeta = SYMMETRICBETA))\n",
    "        ])\n",
    "\n",
    "        X_t = pipe.fit_transform(X_train)\n",
    "        X_v = pipe.fit_transform(X_val)\n",
    "\n",
    "        columns = list(X_t.columns)\n",
    "        cats = [idx for idx, col in enumerate(columns) if categorical_identifier(col)]\n",
    "\n",
    "        regressor_params = {\n",
    "            'device_type' : 'cpu',\n",
    "            'objective' : 'regression',\n",
    "            'n_estimators' : 999_999 if n_estimators is None else n_estimators,\n",
    "            'learning_rate' : 0.02,\n",
    "            'num_leaves' : trial.suggest_int('num_leaves', 4, 2000),\n",
    "            'max_depth' : trial.suggest_int('max_depth', 2, 30),\n",
    "            'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 10, 200, step = 10),\n",
    "            'lambda_l1' : trial.suggest_float('lambda_l1', 1e-6, 10.0, log = True),\n",
    "            'lambda_l2' : trial.suggest_float('lambda_l2', 1e-6, 10.0, log = True),\n",
    "            'min_gain_to_split' : trial.suggest_float('min_gain_to_split', 1e-6, 1.0, log = True),\n",
    "            'bagging_freq' : 1,\n",
    "            'bagging_fraction' : trial.suggest_float('bagging_fraction', 0.1, 0.99),\n",
    "            'feature_fraction' : trial.suggest_float('feature_fraction', 0.05, 1.0),\n",
    "            'min_data_per_group' : trial.suggest_int('min_data_per_group', 1, 10),\n",
    "            'cat_smooth' : trial.suggest_float('cat_smooth', 0, 25.0),\n",
    "            'n_jobs': 6,\n",
    "            'importance_type' : 'gain'\n",
    "        }\n",
    "        # if regressor_params['bagging_freq'] != 0:\n",
    "        #     regressor_params['bagging_fraction'] = trial.suggest_float('bagging_fraction', 0.1, 0.99)\n",
    "\n",
    "        callbacks = [lgbm.log_evaluation(1)]\n",
    "        if n_estimators is None:\n",
    "            callbacks.append(lgbm.early_stopping(250))\n",
    "        if evaluate:\n",
    "            lgbm_logger.info(f'Evaluation Trial')\n",
    "        else:\n",
    "            lgbm_logger.info(f'Trial number : {trial.number}')\n",
    "            callbacks.append(optuna.integration.LightGBMPruningCallback(trial, 'l2', 'valid_0'))\n",
    "        \n",
    "        regression_model = lgbm.LGBMRegressor(**regressor_params)\n",
    "\n",
    "        regression_model.fit(\n",
    "            X_t.to_numpy(),\n",
    "            y_train,\n",
    "            categorical_feature = cats,\n",
    "            eval_set = [(X_v.to_numpy(), y_val)],\n",
    "            eval_metric = 'l2',\n",
    "            feature_name = columns,\n",
    "            callbacks = callbacks\n",
    "        )\n",
    "\n",
    "        if evaluate:\n",
    "            lgbm_logger.info(f\"Best validation RMSE : {regression_model.best_score_['valid_0']['l2']}\")\n",
    "            lgbm_logger.info(f'In-sample r^2 : {regression_model.score(X_t.to_numpy(), y_train)}')\n",
    "            lgbm_logger.info(f'Early stopping validation set r^2 : {regression_model.score(X_v.to_numpy(), y_val)}')\n",
    "            if test_set is not None:\n",
    "                test_data, test_target = test_set\n",
    "                lgbm_logger.info(f'Out-of-sample r^2 : {regression_model.score(pipe.fit_transform(test_data).to_numpy(), test_target)}')\n",
    "            return regression_model\n",
    "\n",
    "        return regression_model.best_score_['valid_0']['l2']\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = objective_factory(X_train, replies_train, X_val, replies_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'replies'\n",
    "\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.FileHandler(f'TreeModels/logs/optuna_target={target}.log'))\n",
    "\n",
    "# study = optuna.create_study(direction = 'minimize', study_name = f'tree_regressor_for_{target}', storage = f\"sqlite:///TreeModels/optuna_study_for_{target}.db\", pruner = optuna.pruners.MedianPruner(n_warmup_steps = 200))\n",
    "study = optuna.create_study(\n",
    "    direction = 'minimize',\n",
    "    study_name = f'tree_regressor_for_{target}',\n",
    "    storage = f\"sqlite:///TreeModels/optuna_study_for_{target}.db\",\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource = 200),\n",
    "    # sampler = optuna.samplers.TPESampler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(objective, n_trials = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study = optuna.create_study(direction = 'minimize', study_name = f'tree_regressor_for_{target}', storage = f\"sqlite:///TreeModels/optuna_study_for_{target}.db\", load_if_exists = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_factory(X_train, y_train, X_val, y_val, evaluate = False, test_set = None, n_estimators = None):\n",
    "\n",
    "    def objective(trial : optuna.Trial):\n",
    "\n",
    "        TIME_ENCODE_TYPE = 'rbf'\n",
    "        N_TIME_GAUSSIANS = 4\n",
    "        SENTIMENT_TYPE = trial.suggest_categorical('sentiment_type', ['none', 'vader', 'distilbert', 'roberta', 'siebert'])\n",
    "        NTOPICS = _nTopics_list[trial.suggest_int('nTopics_INDEX', 0, len(_nTopics_list) - 1)]\n",
    "        TOP_5 = trial.suggest_categorical('top_5', [True, False])\n",
    "        if TOP_5:\n",
    "            INCLUDE_SCORES = trial.suggest_categorical('include_scores', [True, False])\n",
    "        else:\n",
    "            INCLUDE_SCORES = None\n",
    "        SYMMETRICBETA = trial.suggest_categorical('symmetricbeta', [True, False])\n",
    "\n",
    "        DROP_MONTH = trial.suggest_categorical('drop_month', [True, False])\n",
    "        DROP_DAYOFWEEK = trial.suggest_categorical('drop_dayofweek', [True, False])\n",
    "        DROP_COMMENTS = trial.suggest_categorical('drop_comments', [True, False])\n",
    "        DROP_LENGTHS = trial.suggest_categorical('drop_lengths', [True, False])\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            ('section_encoder', IntegerCategoricalEncoder('section')),\n",
    "            ('time_encoder', TimeEncoder(time_encode_type = TIME_ENCODE_TYPE, n_time_gaussians = N_TIME_GAUSSIANS)),\n",
    "            ('drop_columns', ColumnRemover(DROP_MONTH, DROP_DAYOFWEEK, DROP_COMMENTS, DROP_LENGTHS)),\n",
    "            ('sentiment_selector', SentimentSelector(sentiment_type = SENTIMENT_TYPE)),\n",
    "            ('topic_loader', TopicLoader(nTopics = NTOPICS, top_5 = TOP_5, include_scores = INCLUDE_SCORES, symmetricbeta = SYMMETRICBETA))\n",
    "        ])\n",
    "\n",
    "        X_t = pipe.fit_transform(X_train)\n",
    "        X_v = pipe.fit_transform(X_val)\n",
    "\n",
    "        columns = list(X_t.columns)\n",
    "        cats = [idx for idx, col in enumerate(columns) if categorical_identifier(col)]\n",
    "\n",
    "        regressor_params = {\n",
    "            'device_type' : 'cpu',\n",
    "            'objective' : 'regression',\n",
    "            'n_estimators' : 999_999 if n_estimators is None else n_estimators,\n",
    "            'learning_rate' : 0.02,\n",
    "            'num_leaves' : trial.suggest_int('num_leaves', 4, 2000),\n",
    "            'max_depth' : trial.suggest_int('max_depth', 2, 30),\n",
    "            'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 10, 200, step = 10),\n",
    "            'lambda_l1' : trial.suggest_float('lambda_l1', 1e-6, 0.1, log = True),\n",
    "            'lambda_l2' : trial.suggest_float('lambda_l2', 1e-6, 0.1, log = True),\n",
    "            'min_gain_to_split' : trial.suggest_float('min_gain_to_split', 1e-6, 0.1, log = True),\n",
    "            'bagging_freq' : 1,\n",
    "            'bagging_fraction' : trial.suggest_float('bagging_fraction', 0.5, 0.99),\n",
    "            'feature_fraction' : trial.suggest_float('feature_fraction', 0.05, 0.5),\n",
    "            'min_data_per_group' : trial.suggest_int('min_data_per_group', 1, 10),\n",
    "            'cat_smooth' : trial.suggest_float('cat_smooth', 0, 50.0),\n",
    "            'n_jobs': 6,\n",
    "            'importance_type' : 'gain'\n",
    "        }\n",
    "\n",
    "        callbacks = [lgbm.log_evaluation(1)]\n",
    "        if n_estimators is None:\n",
    "            callbacks.append(lgbm.early_stopping(250))\n",
    "        if evaluate:\n",
    "            lgbm_logger.info(f'Evaluation Trial')\n",
    "        else:\n",
    "            lgbm_logger.info(f'Trial number : {trial.number}')\n",
    "            callbacks.append(optuna.integration.LightGBMPruningCallback(trial, 'l2', 'valid_0'))\n",
    "        \n",
    "        regression_model = lgbm.LGBMRegressor(**regressor_params)\n",
    "\n",
    "        regression_model.fit(\n",
    "            X_t.to_numpy(),\n",
    "            y_train,\n",
    "            categorical_feature = cats,\n",
    "            eval_set = [(X_v.to_numpy(), y_val)],\n",
    "            eval_metric = 'l2',\n",
    "            feature_name = columns,\n",
    "            callbacks = callbacks\n",
    "        )\n",
    "\n",
    "        if evaluate:\n",
    "            lgbm_logger.info(f\"Best validation RMSE : {regression_model.best_score_['valid_0']['l2']}\")\n",
    "            lgbm_logger.info(f'In-sample r^2 : {regression_model.score(X_t.to_numpy(), y_train)}')\n",
    "            lgbm_logger.info(f'Early stopping validation set r^2 : {regression_model.score(X_v.to_numpy(), y_val)}')\n",
    "            if test_set is not None:\n",
    "                test_data, test_target = test_set\n",
    "                lgbm_logger.info(f'Out-of-sample r^2 : {regression_model.score(pipe.fit_transform(test_data).to_numpy(), test_target)}')\n",
    "            return regression_model\n",
    "\n",
    "        return regression_model.best_score_['valid_0']['l2']\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = objective_factory(X_train, retweets_train, X_val, retweets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'replies'\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction = 'minimize',\n",
    "    study_name = f'tree_regressor_for_{target}',\n",
    "    storage = f\"sqlite:///TreeModels/optuna_study_for_{target}.db\",\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource = 200),\n",
    "    # sampler = optuna.samplers.TPESampler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(objective, n_trials = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_factory(X_train, y_train, X_val, y_val, evaluate = False, test_set = None, n_estimators = None):\n",
    "\n",
    "    def objective(trial : optuna.Trial):\n",
    "\n",
    "        TIME_ENCODE_TYPE = 'rbf'\n",
    "        N_TIME_GAUSSIANS = 4\n",
    "        SENTIMENT_TYPE = 'roberta'\n",
    "        NTOPICS = _nTopics_list[trial.suggest_int('nTopics_INDEX', 0, len(_nTopics_list) - 1)]\n",
    "        TOP_5 = trial.suggest_categorical('top_5', [True, False])\n",
    "        if TOP_5:\n",
    "            INCLUDE_SCORES = trial.suggest_categorical('include_scores', [True, False])\n",
    "        else:\n",
    "            INCLUDE_SCORES = None\n",
    "        SYMMETRICBETA = trial.suggest_categorical('symmetricbeta', [True, False])\n",
    "\n",
    "        DROP_MONTH = False\n",
    "        DROP_DAYOFWEEK = False\n",
    "        DROP_COMMENTS = True\n",
    "        DROP_LENGTHS = False\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            ('section_encoder', IntegerCategoricalEncoder('section')),\n",
    "            ('time_encoder', TimeEncoder(time_encode_type = TIME_ENCODE_TYPE, n_time_gaussians = N_TIME_GAUSSIANS)),\n",
    "            ('drop_columns', ColumnRemover(DROP_MONTH, DROP_DAYOFWEEK, DROP_COMMENTS, DROP_LENGTHS)),\n",
    "            ('sentiment_selector', SentimentSelector(sentiment_type = SENTIMENT_TYPE)),\n",
    "            ('topic_loader', TopicLoader(nTopics = NTOPICS, top_5 = TOP_5, include_scores = INCLUDE_SCORES, symmetricbeta = SYMMETRICBETA))\n",
    "        ])\n",
    "\n",
    "        X_t = pipe.fit_transform(X_train)\n",
    "        X_v = pipe.fit_transform(X_val)\n",
    "\n",
    "        columns = list(X_t.columns)\n",
    "        cats = [idx for idx, col in enumerate(columns) if categorical_identifier(col)]\n",
    "\n",
    "        regressor_params = {\n",
    "            'device_type' : 'cpu',\n",
    "            'objective' : 'regression',\n",
    "            'n_estimators' : 999_999 if n_estimators is None else n_estimators,\n",
    "            'learning_rate' : 0.02,\n",
    "            'num_leaves' : trial.suggest_int('num_leaves', 4, 2000),\n",
    "            'max_depth' : trial.suggest_int('max_depth', 2, 20),\n",
    "            'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 5, 50, step = 5),\n",
    "            'lambda_l1' : trial.suggest_float('lambda_l1', 1e-6, 0.1, log = True),\n",
    "            'lambda_l2' : trial.suggest_float('lambda_l2', 1e-6, 0.1, log = True),\n",
    "            'min_gain_to_split' : trial.suggest_float('min_gain_to_split', 1e-6, 0.1, log = True),\n",
    "            'bagging_freq' : 1,\n",
    "            'bagging_fraction' : 0.95,\n",
    "            'feature_fraction' : 0.15,\n",
    "            'min_data_per_group' : trial.suggest_int('min_data_per_group', 1, 15),\n",
    "            'cat_smooth' : trial.suggest_float('cat_smooth', 0, 100.0),\n",
    "            'max_cat_threshold' : trial.suggest_int('max_cat_threshold', 1, 50),\n",
    "            'cat_l2' : trial.suggest_float('cat_l2', 1e-8, 10.0, log = True),\n",
    "            'max_cat_to_onehot' : trial.suggest_int('max_cat_to_onehot', 1, 10),\n",
    "            'n_jobs': 6,\n",
    "            'importance_type' : 'gain'\n",
    "        }\n",
    "\n",
    "        callbacks = [lgbm.log_evaluation(1)]\n",
    "        if n_estimators is None:\n",
    "            callbacks.append(lgbm.early_stopping(250))\n",
    "        if evaluate:\n",
    "            lgbm_logger.info(f'Evaluation Trial')\n",
    "        else:\n",
    "            lgbm_logger.info(f'Trial number : {trial.number}')\n",
    "            callbacks.append(optuna.integration.LightGBMPruningCallback(trial, 'l2', 'valid_0'))\n",
    "        \n",
    "        regression_model = lgbm.LGBMRegressor(**regressor_params)\n",
    "\n",
    "        regression_model.fit(\n",
    "            X_t.to_numpy(),\n",
    "            y_train,\n",
    "            categorical_feature = cats,\n",
    "            eval_set = [(X_v.to_numpy(), y_val)],\n",
    "            eval_metric = 'l2',\n",
    "            feature_name = columns,\n",
    "            callbacks = callbacks\n",
    "        )\n",
    "\n",
    "        if evaluate:\n",
    "            lgbm_logger.info(f\"Best validation RMSE : {regression_model.best_score_['valid_0']['l2']}\")\n",
    "            lgbm_logger.info(f'In-sample r^2 : {regression_model.score(X_t.to_numpy(), y_train)}')\n",
    "            lgbm_logger.info(f'Early stopping validation set r^2 : {regression_model.score(X_v.to_numpy(), y_val)}')\n",
    "            if test_set is not None:\n",
    "                test_data, test_target = test_set\n",
    "                lgbm_logger.info(f'Out-of-sample r^2 : {regression_model.score(pipe.fit_transform(test_data).to_numpy(), test_target)}')\n",
    "            return regression_model\n",
    "\n",
    "        return regression_model.best_score_['valid_0']['l2']\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = objective_factory(X_train, replies_train, X_val, replies_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'replies'\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction = 'minimize',\n",
    "    study_name = f'tree_regressor_for_{target}',\n",
    "    storage = f\"sqlite:///TreeModels/optuna_study_for_{target}.db\",\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource = 200),\n",
    "    # sampler = optuna.samplers.TPESampler(),\n",
    "    load_if_exists = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(objective, n_trials = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trial = objective_factory(X_train, replies_train, X_val, replies_val, evaluate = True, test_set = (X_test, replies_test))\n",
    "m = eval_trial(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trial = objective_factory(X_combined, retweets_combined, X_combined, retweets_combined, evaluate = True, test_set = (X_test, retweets_test), n_estimators = 963)\n",
    "m = eval_trial(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52dc165078b550922a92e20afc6187e50fa6838252eb052b89f3983eda27ca00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
