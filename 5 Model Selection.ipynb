{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "from typing import List, Tuple\n",
    "import logging\n",
    "from itertools import repeat, chain\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import randint, loguniform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "import tables\n",
    "\n",
    "import shap\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# from ediblepickle import checkpoint\n",
    "\n",
    "import lightgbm as lgbm\n",
    "\n",
    "# from ray import tune\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Get the training and validation datasets from an SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_query = sql.SQL(\"\"\"SELECT ttv.id, tweets.retweet_count, tweets.like_count, tweets.reply_count,\n",
    "    \n",
    "                        tweets.video AS tweet_has_video,\n",
    "                        tweets.photo AS tweet_has_photo,\n",
    "\n",
    "                        articles.video AS article_has_video,\n",
    "                        articles.audio AS article_has_audio,\n",
    "                        articles.comments,\n",
    "\n",
    "                        textlengths.tweetlength,\n",
    "                        textlengths.titlelength,\n",
    "                        textlengths.summarylength,\n",
    "                        textlengths.articlelength,\n",
    "\n",
    "                        sections.section,\n",
    "\n",
    "                        timeinfo.seconds,\n",
    "                        timeinfo.month,\n",
    "                        timeinfo.dayofweek,\n",
    "                        \n",
    "                        sentiment.vader_tweet_texts_neg,\n",
    "                        sentiment.vader_tweet_texts_neu,\n",
    "                        sentiment.vader_tweet_texts_pos,\n",
    "                        sentiment.vader_tweet_texts_compound,\n",
    "                        sentiment.vader_article_titles_neg,\n",
    "                        sentiment.vader_article_titles_neu,\n",
    "                        sentiment.vader_article_titles_pos,\n",
    "                        sentiment.vader_article_titles_compound,\n",
    "                        sentiment.vader_article_summaries_neg,\n",
    "                        sentiment.vader_article_summaries_neu,\n",
    "                        sentiment.vader_article_summaries_pos,\n",
    "                        sentiment.vader_article_summaries_compound,\n",
    "                        sentiment.vader_article_main_neg,\n",
    "                        sentiment.vader_article_main_neu,\n",
    "                        sentiment.vader_article_main_pos,\n",
    "                        sentiment.vader_article_main_compound,\n",
    "\n",
    "                        sentiment.distilbert_tweet_texts_negative,\n",
    "                        sentiment.distilbert_tweet_texts_positive,\n",
    "                        sentiment.distilbert_article_titles_negative,\n",
    "                        sentiment.distilbert_article_titles_positive,\n",
    "                        sentiment.distilbert_article_summaries_negative,\n",
    "                        sentiment.distilbert_article_summaries_positive,\n",
    "                        sentiment.distilbert_article_main_negative,\n",
    "                        sentiment.distilbert_article_main_positive,\n",
    "\n",
    "                        sentiment.roberta_tweet_texts_negative,\n",
    "                        sentiment.roberta_tweet_texts_positive,\n",
    "                        sentiment.roberta_tweet_texts_neutral,\n",
    "                        sentiment.roberta_article_titles_negative,\n",
    "                        sentiment.roberta_article_titles_positive,\n",
    "                        sentiment.roberta_article_titles_neutral,\n",
    "                        sentiment.roberta_article_summaries_negative,\n",
    "                        sentiment.roberta_article_summaries_positive,\n",
    "                        sentiment.roberta_article_summaries_neutral,\n",
    "                        sentiment.roberta_article_main_negative,\n",
    "                        sentiment.roberta_article_main_positive,\n",
    "                        sentiment.roberta_article_main_neutral,\n",
    "\n",
    "                        sentiment.siebert_tweet_texts_negative,\n",
    "                        sentiment.siebert_tweet_texts_positive,\n",
    "                        sentiment.siebert_article_titles_negative,\n",
    "                        sentiment.siebert_article_titles_positive,\n",
    "                        sentiment.siebert_article_summaries_negative,\n",
    "                        sentiment.siebert_article_summaries_positive,\n",
    "                        sentiment.siebert_article_main_negative,\n",
    "                        sentiment.siebert_article_main_positive\n",
    "\n",
    "                    FROM (SELECT id FROM traintest WHERE split = {}) AS ttv\n",
    "                    INNER JOIN tweets ON ttv.id = tweets.id\n",
    "                    INNER JOIN articles ON ttv.id = articles.id\n",
    "                    INNER JOIN textlengths ON ttv.id = textlengths.id\n",
    "                    INNER JOIN sections ON ttv.id = sections.id\n",
    "                    INNER JOIN timeinfo ON ttv.id = timeinfo.id\n",
    "                    INNER JOIN sentiment ON ttv.id = sentiment.id\n",
    "                    WHERE tweets.date < {}\n",
    "                    ;\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_DATE = '2022-04-16'\n",
    "\n",
    "with psycopg2.connect(host = 'localhost', database = 'nytpopular') as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(db_query.format(sql.Literal('train'), sql.Literal(CUTOFF_DATE)))\n",
    "        train = cursor.fetchall()\n",
    "        train_column_names = [description[0] for description in cursor.description]\n",
    "        cursor.execute(db_query.format(sql.Literal('valid'), sql.Literal(CUTOFF_DATE)))\n",
    "        val = cursor.fetchall()\n",
    "        val_column_names = [description[0] for description in cursor.description]\n",
    "        cursor.execute(db_query.format(sql.Literal('test'), sql.Literal(CUTOFF_DATE)))\n",
    "        test = cursor.fetchall()\n",
    "        test_column_names = [description[0] for description in cursor.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_target(target : pd.Series) -> pd.Series:\n",
    "    return np.log10(target + 1.0)\n",
    "\n",
    "def preprocess(dataset : List[Tuple], columns : List[str]) -> pd.DataFrame:\n",
    "    X = pd.DataFrame(dataset, columns = columns).set_index('id')\n",
    "    likes = transform_target(X['like_count'])\n",
    "    retweets = transform_target(X['retweet_count'])\n",
    "    replies = transform_target(X['reply_count'])\n",
    "    X = X.drop(['like_count', 'retweet_count', 'reply_count'], axis = 1)\n",
    "    for key in ['tweet_has_video', 'tweet_has_photo', 'article_has_video', 'article_has_audio']: # 0/1 encode these bools\n",
    "        X[key] = X[key].astype(int)\n",
    "    # Were comments enabled? The number of comments cannot be kept as a feature because this is not information that is available before an article is published.\n",
    "    # It's possible that \n",
    "    X['comments'] = (X['comments']/X['comments']).fillna(0).astype(int)\n",
    "    return X, likes, retweets, replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_sections(X): # Danger: intentionally mutates the input!!!\n",
    "    section_counts = X['section'].value_counts()\n",
    "    MIN_COUNT = 2\n",
    "    sections_to_be_removed = section_counts[section_counts <= MIN_COUNT].index\n",
    "    X.loc[X['section'].isin(sections_to_be_removed), 'section'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, likes_train, retweets_train, replies_train = preprocess(train, train_column_names)\n",
    "X_val, likes_val, retweets_val, replies_val = preprocess(val, val_column_names)\n",
    "X_test, likes_test, retweets_test, replies_test = preprocess(test, test_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rare_sections(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_val_combined = pd.concat([X_train, X_val])\n",
    "X_combined, likes_combined, retweets_combined, replies_combined = preprocess([*train, *val], train_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy in ['mean', 'median']:\n",
    "    for label, y_combined, y_test in zip(['likes', 'retweets', 'replies'], [likes_combined, retweets_combined, replies_combined], [likes_test, retweets_test, replies_test]):\n",
    "        baseline = DummyRegressor(strategy=strategy)\n",
    "        baseline.fit(X_combined, y_combined)\n",
    "        print(f'R^2 for the {strategy} model for {label} is {baseline.score(X_test, y_test)}.') # Should be near 0, since R^2 compares the model to the mean model (i.e. exactly zero for the training data).\n",
    "    print()\n",
    "\n",
    "# A little surprising to me that the median model is actually slightly worse than the mean model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pipeline\n",
    "\n",
    "Pipeline transformers for selecting features and getting the data into the correct format.\n",
    "\n",
    "I will use LightGBM for tree-based gradient boosting because LightGBM is accurate, fast, handles categorical variables without needed to one-hot encode, and handles missing data (although how missing data is handled is [not always intelligent](https://github.com/microsoft/LightGBM/issues/2921).\n",
    "\n",
    "Things to watch out for:\n",
    "- Although LightGBM can handle categorical features in pandas DataFrames, this is potentially dangerous if new unseen categories appear after training. I will use scikit-learn to encode categorical data into integers.\n",
    "- Although multicollinearity will not significantly adversely affect predictions, it may make interpretation difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegerCategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, column_name : str, only_this : bool = False):\n",
    "        self.column_name = column_name\n",
    "        self.only_this = only_this\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        # encoded_missing_value is new in scikit-learn 1.1. Remove it for older versions.\n",
    "        self._ordinalencoder = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = np.nan, encoded_missing_value = np.nan)\n",
    "        self._ordinalencoder.fit(X[[self.column_name]])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.only_this:\n",
    "            return pd.DataFrame(self._ordinalencoder.transform(X[[self.column_name]]), columns=[self.column_name])\n",
    "        X = X.copy()\n",
    "        X[self.column_name] = self._ordinalencoder.transform(X[[self.column_name]])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DAILY_SECONDS = 86400\n",
    "HOURLY_SECONDS = 3600\n",
    "\n",
    "def gaussian(x, center, width):\n",
    "    return np.exp(-(x - center)**2 / (2 * width **2))\n",
    "\n",
    "def periodic_gaussian(x, center_in_hours, width_in_hours): # Not actually periodic, and can be > 1\n",
    "    center = center_in_hours * HOURLY_SECONDS\n",
    "    width = width_in_hours * HOURLY_SECONDS\n",
    "    return gaussian(x - DAILY_SECONDS, center, width) + gaussian(x, center, width) + gaussian(x + DAILY_SECONDS, center, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, time_encode_type : str, n_time_gaussians = None, only_this = False):\n",
    "        if time_encode_type == 'raw':\n",
    "            assert n_time_gaussians is None, 'n_time_gaussians specified when time_encode_type == \"raw\"'\n",
    "        elif time_encode_type == 'rbf': # Radial Basis Functions\n",
    "            assert n_time_gaussians in [4, 6, 12, 24], 'n_time_gaussians must divide 24'\n",
    "            self.n_time_gaussians = n_time_gaussians\n",
    "        else:\n",
    "            assert False, 'Unknown time_encode_type'\n",
    "        self.time_encode_type = time_encode_type\n",
    "        self.only_this = only_this\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.time_encode_type == 'rbf':\n",
    "            seconds = X['seconds']\n",
    "            X = X.drop(['seconds'], axis = 1)\n",
    "            for basis_index in range(self.n_time_gaussians):\n",
    "                hour = basis_index * 24 // self.n_time_gaussians\n",
    "                # A measure of how far away the time is from the \"hour\". ~1 represents right on the hour, ~0 represents far away from the hour.\n",
    "                # Should have probably made width_in_hours an independent tunable hyperparameter to search for instead of fixing it to self.n_time_gaussians, but oh well...\n",
    "                X[f'hour_{hour}'] = periodic_gaussian(seconds, hour, 24 // self.n_time_gaussians)\n",
    "        if self.only_this:\n",
    "            time_columns = [col for col in X.columns if any(time_word in col.lower() for time_word in ['hour', 'month', 'week', 'seconds'])]\n",
    "            X = X[time_columns]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, sentiment_type : str, only_this : bool = False):\n",
    "        self._sentiment_models = {'none', 'vader', 'distilbert', 'roberta', 'siebert'}\n",
    "        assert sentiment_type in self._sentiment_models, 'Unknown sentiment analyzer'\n",
    "        self.sentiment_type = sentiment_type\n",
    "        self.only_this = only_this\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.only_this:\n",
    "            wanted_columns = [column for column in X.columns if self.sentiment_type in column]\n",
    "            return X[wanted_columns]\n",
    "        unwanted_columns = [column for column in X.columns if any(sentiment_type in column for sentiment_type in self._sentiment_models.difference({self.sentiment_type}))]\n",
    "        X = X.drop(unwanted_columns, axis = 1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnRemover(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, drop_month = False, drop_dayofweek = False, drop_comments = False, drop_lengths = False):\n",
    "        self.drop_month = drop_month\n",
    "        self.drop_dayofweek = drop_dayofweek\n",
    "        self.drop_comments = drop_comments\n",
    "        self.drop_lengths = drop_lengths\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        unwanted_columns = []\n",
    "        if self.drop_month:\n",
    "            unwanted_columns.append('month')\n",
    "        if self.drop_dayofweek:\n",
    "            unwanted_columns.append('dayofweek')\n",
    "        if self.drop_comments:\n",
    "            unwanted_columns.append('comments')\n",
    "        if self.drop_lengths:\n",
    "            unwanted_columns.extend([column for column in X.columns if 'length' in column])\n",
    "        X = X.drop(unwanted_columns, axis = 1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_nTopics_list = list(chain(range(5,30,5),range(30, 200 + 1, 10)))\n",
    "_valid_nTopics = set(_nTopics_list)\n",
    "\n",
    "def load_topic_df(filename : str, nTopics : int, top_topics : bool = False, include_scores = False, MAX_TOPICS = 5, table_suffix : str = '') -> pd.DataFrame:\n",
    "    with tables.open_file(filename, mode = 'r') as f:\n",
    "        table = f.root.topic[f'ntopics{nTopics}' + table_suffix]\n",
    "        ids, topicvectors = zip(*[(x['id'], x['topicvector']) for x in table.iterrows()])\n",
    "        if top_topics:\n",
    "            topic_ranks = [(-tv).argsort()[:MAX_TOPICS] for tv in topicvectors]\n",
    "            if include_scores:\n",
    "                # sorted_topic_scores = [np.sort(tv)[::-1] for tv in topicvectors]\n",
    "                sorted_topic_scores = [[*tr, *tv[tr]] for tr, tv in zip(topic_ranks, topicvectors)]\n",
    "                return pd.DataFrame(sorted_topic_scores, index = ids, columns = [f'best_topic_{n:03}' for n in range(MAX_TOPICS)] + [f'best_topic_scores_{n:03}' for n in range(MAX_TOPICS)])\n",
    "            else:\n",
    "                return pd.DataFrame(topic_ranks, index = ids, columns = [f'best_topic_{n:03}' for n in range(MAX_TOPICS)])\n",
    "        else:\n",
    "            return pd.DataFrame(topicvectors, index = ids, columns = [f'topic_{n:03}' for n in range(nTopics)])\n",
    "\n",
    "class TopicLoader(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, nTopics : int, top_5 : bool = False, include_scores : bool = False, symmetricbeta : bool = False, only_this : bool = False, pca = False, filename : str = None, table_suffix : str = ''):\n",
    "        assert nTopics in _valid_nTopics, 'Invalid number of topics'\n",
    "        assert (not include_scores) or top_5, 'include_scores provided when top_5 is False'\n",
    "        self.nTopics = nTopics\n",
    "        self.top_5 = top_5\n",
    "        self.include_scores = include_scores\n",
    "        self.symmetricbeta = symmetricbeta\n",
    "        if filename is None:\n",
    "            if symmetricbeta:\n",
    "                filename = r'GensimModels/article_data_symmetricbeta.h5'\n",
    "            else:\n",
    "                filename = r'GensimModels/article_data.h5'\n",
    "        self.filename = filename\n",
    "        self.table_suffix = table_suffix\n",
    "        self.pca = pca\n",
    "        self.topics_df = load_topic_df(filename, nTopics, top_5, include_scores, table_suffix = table_suffix)\n",
    "        self.only_this = only_this\n",
    "\n",
    "    def fit(self, X, y = None):\n",
    "        if self.pca != False:\n",
    "            assert self.top_5 == False, 'top_5 cannot be True if pca == True'\n",
    "            assert isinstance(self.pca, int), 'pca must be an int'\n",
    "            assert self.pca < self.nTopics, 'pca must be smaller than nTopics'\n",
    "            self._pca_transformer = PCA(n_components = self.pca)\n",
    "            self._pca_transformer.fit(self.topics_df.loc[X.index, :])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        topics_data = self.topics_df.loc[X.index, :]\n",
    "        if self.pca != False:\n",
    "            topics_data = pd.DataFrame(self._pca_transformer.transform(topics_data), columns = [f'topic_pca_{component_index:03}' for component_index in range(self.pca)], index = X.index)\n",
    "        if self.only_this:\n",
    "            return topics_data\n",
    "        return pd.merge(X, topics_data, how = 'inner', left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_topic_regex = re.compile(r'^best_topic_\\d{3}$')\n",
    "\n",
    "def categorical_identifier(label : str) -> bool:\n",
    "    if label in {'section', 'month', 'dayofweek', 'comments', 'article_has_audio', 'article_has_video', 'tweet_has_video', 'tweet_has_photo'}:\n",
    "        return True\n",
    "    elif best_topic_regex.match(label):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "I use Optuna for hyperparameter optimization. Ideally, the data would be split into a training set, two validation sets (one for hyperparameter tuning and the other for determining early stopping), and a test set (which is only used for model evaluation and never used for model selection). Since I have limited data (and scraping more data may not help because news topics probably go in and out of fashion), I will just use the same validation set for hyperparameter tuning and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_logger = logging.getLogger('lgbm')\n",
    "loghandle = logging.FileHandler(f'TreeModels/logs/lgbm.log')\n",
    "logformat = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "loghandle.setFormatter(logformat)\n",
    "lgbm_logger.addHandler(loghandle)\n",
    "lgbm_logger.setLevel(logging.INFO)\n",
    "\n",
    "lgbm.register_logger(lgbm_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_factory(X_train, y_train, X_val, y_val, evaluate = False, test_set = None, n_estimators = None):\n",
    "\n",
    "    def objective(trial : optuna.Trial):\n",
    "\n",
    "        TIME_ENCODE_TYPE = 'rbf'\n",
    "        N_TIME_GAUSSIANS = 4\n",
    "        SENTIMENT_TYPE = 'none'\n",
    "        NTOPICS = 130\n",
    "        TOP_5 = False\n",
    "        INCLUDE_SCORES = None\n",
    "        SYMMETRICBETA = True\n",
    "\n",
    "        DROP_MONTH = True\n",
    "        DROP_DAYOFWEEK = False\n",
    "        DROP_COMMENTS = False\n",
    "        DROP_LENGTHS = False\n",
    "\n",
    "        # trial.suggest_int('pca', 2, 120)\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            ('section_encoder', IntegerCategoricalEncoder('section')),\n",
    "            ('time_encoder', TimeEncoder(time_encode_type = TIME_ENCODE_TYPE, n_time_gaussians = N_TIME_GAUSSIANS)),\n",
    "            ('drop_columns', ColumnRemover(DROP_MONTH, DROP_DAYOFWEEK, DROP_COMMENTS, DROP_LENGTHS)),\n",
    "            ('sentiment_selector', SentimentSelector(sentiment_type = SENTIMENT_TYPE)),\n",
    "            ('topic_loader', TopicLoader(nTopics = NTOPICS, top_5 = TOP_5, include_scores = INCLUDE_SCORES, symmetricbeta = SYMMETRICBETA))\n",
    "        ])\n",
    "\n",
    "        X_t = pipe.fit_transform(X_train)\n",
    "        X_v = pipe.fit_transform(X_val)\n",
    "\n",
    "        columns = list(X_t.columns)\n",
    "        cats = [idx for idx, col in enumerate(columns) if categorical_identifier(col)]\n",
    "\n",
    "        regressor_params = {\n",
    "            'device_type' : 'cpu',\n",
    "            'objective' : 'regression',\n",
    "            'n_estimators' : 999_999 if n_estimators is None else n_estimators,\n",
    "            'learning_rate' : 0.02,\n",
    "            'num_leaves' : trial.suggest_int('num_leaves', 4, 2000),\n",
    "            'max_depth' : trial.suggest_int('max_depth', 2, 20),\n",
    "            'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 5, 50, step = 5),\n",
    "            'lambda_l1' : trial.suggest_float('lambda_l1', 1e-6, 0.1, log = True),\n",
    "            'lambda_l2' : trial.suggest_float('lambda_l2', 1e-6, 0.1, log = True),\n",
    "            'min_gain_to_split' : trial.suggest_float('min_gain_to_split', 1e-6, 0.1, log = True),\n",
    "            'bagging_freq' : 1,\n",
    "            'bagging_fraction' : 0.95,\n",
    "            'feature_fraction' : 0.15,\n",
    "            'min_data_per_group' : trial.suggest_int('min_data_per_group', 1, 15),\n",
    "            'cat_smooth' : trial.suggest_float('cat_smooth', 0, 100.0),\n",
    "            'max_cat_threshold' : trial.suggest_int('max_cat_threshold', 1, 50),\n",
    "            'cat_l2' : trial.suggest_float('cat_l2', 1e-8, 10.0, log = True),\n",
    "            'max_cat_to_onehot' : trial.suggest_int('max_cat_to_onehot', 1, 10),\n",
    "            'n_jobs': 6,\n",
    "            'importance_type' : 'gain',\n",
    "            # 'random_state' : 137\n",
    "        }\n",
    "\n",
    "        callbacks = [lgbm.log_evaluation(1)]\n",
    "        if n_estimators is None:\n",
    "            callbacks.append(lgbm.early_stopping(250))\n",
    "        if evaluate:\n",
    "            lgbm_logger.info(f'Evaluation Trial')\n",
    "        else:\n",
    "            lgbm_logger.info(f'Trial number : {trial.number}')\n",
    "            callbacks.append(optuna.integration.LightGBMPruningCallback(trial, 'l2', 'valid_0'))\n",
    "        \n",
    "        regression_model = lgbm.LGBMRegressor(**regressor_params)\n",
    "\n",
    "        regression_model.fit(\n",
    "            X_t.to_numpy(),\n",
    "            y_train,\n",
    "            categorical_feature = cats,\n",
    "            eval_set = [(X_v.to_numpy(), y_val)],\n",
    "            eval_metric = 'l2',\n",
    "            feature_name = columns,\n",
    "            callbacks = callbacks\n",
    "        )\n",
    "\n",
    "        if evaluate:\n",
    "            lgbm_logger.info(f'Best iteration : {regression_model.best_iteration_}')\n",
    "            lgbm_logger.info(f\"Best validation RMSE : {regression_model.best_score_['valid_0']['l2']}\")\n",
    "            lgbm_logger.info(f'In-sample r^2 : {regression_model.score(X_t.to_numpy(), y_train)}')\n",
    "            lgbm_logger.info(f'Early stopping validation set r^2 : {regression_model.score(X_v.to_numpy(), y_val)}')\n",
    "            if test_set is not None:\n",
    "                test_data, test_target = test_set\n",
    "                lgbm_logger.info(f'Out-of-sample r^2 : {regression_model.score(pipe.fit_transform(test_data).to_numpy(), test_target)}')\n",
    "            return (pipe, regression_model)\n",
    "\n",
    "        return regression_model.best_score_['valid_0']['l2']\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = objective_factory(X_train, likes_train, X_val, likes_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'likes'\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction = 'minimize',\n",
    "    study_name = f'tree_regressor_for_{target}',\n",
    "    storage = f\"sqlite:///TreeModels/optuna_study_for_{target}_none.db\",\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource = 200),\n",
    "    # sampler = optuna.samplers.TPESampler(),\n",
    "    load_if_exists = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.optimize(objective, n_trials = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'likes'\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction = 'minimize',\n",
    "    study_name = f'tree_regressor_for_{target}',\n",
    "    storage = f\"sqlite:///TreeModels/optuna_study_for_{target}_none.db\",\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource = 200),\n",
    "    # sampler = optuna.samplers.TPESampler(),\n",
    "    load_if_exists = True\n",
    ")\n",
    "\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trial = objective_factory(X_train, likes_train, X_val, likes_val, evaluate = True, test_set = (X_test, likes_test))\n",
    "_ = eval_trial(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trial = objective_factory(X_combined, likes_combined, X_combined, likes_combined, evaluate = True, test_set = (X_test, likes_test), n_estimators = 2461)\n",
    "likes_pipe, likes_regressor = eval_trial(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likes_regressor = lgbm.Booster(model_file = 'TreeModels/lgbm_likes.model')\n",
    "# likes_pipe = joblib.load('TreeModels/likes_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,7))\n",
    "y_actual = likes_test\n",
    "y_pred = likes_regressor.predict(likes_pipe.transform(X_test).to_numpy())\n",
    "ax.scatter(y_actual, y_pred)\n",
    "ax.plot(y_actual, y_actual, color = 'r')\n",
    "ax.set_xlabel(r'$\\log_{10}({\\rm Actual\\ Likes})$', fontsize = 20)\n",
    "ax.set_ylabel(r'$\\log_{10}({\\rm Predicted\\ Likes})$', fontsize = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(likes_pipe, 'TreeModels/likes_pipeline.joblib')\n",
    "likes_regressor.booster_.save_model('TreeModels/lgbm_likes.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'retweets'\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction = 'minimize',\n",
    "    study_name = f'tree_regressor_for_{target}',\n",
    "    storage = f\"sqlite:///TreeModels/optuna_study_for_{target}_none.db\",\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource = 200),\n",
    "    # sampler = optuna.samplers.TPESampler(),\n",
    "    load_if_exists = True\n",
    ")\n",
    "\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trial = objective_factory(X_train, retweets_train, X_val, retweets_val, evaluate = True, test_set = (X_test, retweets_test))\n",
    "_ = eval_trial(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trial = objective_factory(X_combined, retweets_combined, X_combined, retweets_combined, evaluate = True, test_set = (X_test, retweets_test), n_estimators = 1831)\n",
    "retweets_pipe, retweets_regressor = eval_trial(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retweets_regressor = lgbm.Booster(model_file = 'TreeModels/retweets.model')\n",
    "# retweets_pipe = joblib.load('TreeModels/retweets_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,7))\n",
    "y_actual = retweets_test\n",
    "y_pred = retweets_regressor.predict(retweets_pipe.transform(X_test).to_numpy())\n",
    "ax.scatter(y_actual, y_pred)\n",
    "ax.plot(y_actual, y_actual, color = 'r')\n",
    "ax.set_xlabel(r'$\\log_{10}({\\rm Actual\\ Retweets})$', fontsize = 20)\n",
    "ax.set_ylabel(r'$\\log_{10}({\\rm Predicted\\ Retweets})$', fontsize = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(retweets_pipe, 'TreeModels/retweets_pipeline.joblib')\n",
    "retweets_regressor.booster_.save_model('TreeModels/retweets.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'replies'\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction = 'minimize',\n",
    "    study_name = f'tree_regressor_for_{target}',\n",
    "    storage = f\"sqlite:///TreeModels/optuna_study_for_{target}_none.db\",\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource = 200),\n",
    "    # sampler = optuna.samplers.TPESampler(),\n",
    "    load_if_exists = True\n",
    ")\n",
    "\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trial = objective_factory(X_train, replies_train, X_val, replies_val, evaluate = True, test_set = (X_test, replies_test))\n",
    "_ = eval_trial(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trial = objective_factory(X_combined, replies_combined, X_combined, replies_combined, evaluate = True, test_set = (X_test, replies_test), n_estimators = 2262)\n",
    "replies_pipe, replies_regressor = eval_trial(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_regressor = lgbm.Booster(model_file = 'TreeModels/replies.model')\n",
    "replies_pipe = joblib.load('TreeModels/replies_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10,7))\n",
    "y_actual = replies_test\n",
    "y_pred = replies_regressor.predict(replies_pipe.transform(X_test).to_numpy())\n",
    "ax.scatter(y_actual, y_pred)\n",
    "ax.plot(y_actual, y_actual, color = 'r')\n",
    "ax.set_xlabel(r'$\\log_{10}({\\rm Actual\\ Replies})$', fontsize = 20)\n",
    "ax.set_ylabel(r'$\\log_{10}({\\rm Predicted\\ Replies})$', fontsize = 20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(replies_pipe, 'TreeModels/replies_pipeline.joblib')\n",
    "replies_regressor.booster_.save_model('TreeModels/replies.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_on_likes_regressor = LinearRegression()\n",
    "retweets_on_likes_regressor.fit(likes_combined.to_numpy().reshape(-1, 1), retweets_combined)\n",
    "print(retweets_on_likes_regressor.score(likes_combined.to_numpy().reshape(-1, 1), retweets_combined))\n",
    "print(r2_score(retweets_test, retweets_on_likes_regressor.predict(likes_regressor.predict(likes_pipe.transform(X_test).to_numpy()).reshape(-1, 1))))\n",
    "\n",
    "replies_on_likes_regressor = LinearRegression()\n",
    "replies_on_likes_regressor.fit(likes_combined.to_numpy().reshape(-1, 1), replies_combined)\n",
    "print(replies_on_likes_regressor.score(likes_combined.to_numpy().reshape(-1, 1), replies_combined))\n",
    "print(r2_score(replies_test, replies_on_likes_regressor.predict(likes_regressor.predict(likes_pipe.transform(X_test).to_numpy()).reshape(-1, 1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating topic model containing full data\n",
    "\n",
    "In the previous sections of this notebook, models were evaluated against test and validation data that were completely absent from all training steps (including the LDA model).\n",
    "\n",
    "Here, I evaluate models where the gradient-boosted trees do not see the test data, but the LDA model is trained on the full dataset. This gives a slightly optimistic generalization error, but I am doing this to make sure nothing goes wrong when the LDA model is trained on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_factory(X_train, y_train, X_val, y_val, evaluate = False, test_set = None, n_estimators = None, index = 0):\n",
    "\n",
    "    def objective(trial : optuna.Trial):\n",
    "\n",
    "        TIME_ENCODE_TYPE = 'rbf'\n",
    "        N_TIME_GAUSSIANS = 4\n",
    "        SENTIMENT_TYPE = 'none'\n",
    "        NTOPICS = 130\n",
    "        TOP_5 = False\n",
    "        INCLUDE_SCORES = None\n",
    "        SYMMETRICBETA = True\n",
    "\n",
    "        DROP_MONTH = True\n",
    "        DROP_DAYOFWEEK = False\n",
    "        DROP_COMMENTS = False\n",
    "        DROP_LENGTHS = False\n",
    "\n",
    "        # trial.suggest_int('pca', 2, 120)\n",
    "\n",
    "        pipe = Pipeline([\n",
    "            ('section_encoder', IntegerCategoricalEncoder('section')),\n",
    "            ('time_encoder', TimeEncoder(time_encode_type = TIME_ENCODE_TYPE, n_time_gaussians = N_TIME_GAUSSIANS)),\n",
    "            ('drop_columns', ColumnRemover(DROP_MONTH, DROP_DAYOFWEEK, DROP_COMMENTS, DROP_LENGTHS)),\n",
    "            ('sentiment_selector', SentimentSelector(sentiment_type = SENTIMENT_TYPE)),\n",
    "            ('topic_loader', TopicLoader(nTopics = NTOPICS, top_5 = TOP_5, include_scores = INCLUDE_SCORES, symmetricbeta = SYMMETRICBETA, filename = 'FullModels/article_data.h5', table_suffix = f'_index{index}'))\n",
    "        ])\n",
    "\n",
    "        X_t = pipe.fit_transform(X_train)\n",
    "        X_v = pipe.fit_transform(X_val)\n",
    "\n",
    "        columns = list(X_t.columns)\n",
    "        cats = [idx for idx, col in enumerate(columns) if categorical_identifier(col)]\n",
    "\n",
    "        regressor_params = {\n",
    "            'device_type' : 'cpu',\n",
    "            'objective' : 'regression',\n",
    "            'n_estimators' : 999_999 if n_estimators is None else n_estimators,\n",
    "            'learning_rate' : 0.02,\n",
    "            'num_leaves' : trial.suggest_int('num_leaves', 4, 2000),\n",
    "            'max_depth' : trial.suggest_int('max_depth', 2, 20),\n",
    "            'min_data_in_leaf' : trial.suggest_int('min_data_in_leaf', 5, 50, step = 5),\n",
    "            'lambda_l1' : trial.suggest_float('lambda_l1', 1e-6, 0.1, log = True),\n",
    "            'lambda_l2' : trial.suggest_float('lambda_l2', 1e-6, 0.1, log = True),\n",
    "            'min_gain_to_split' : trial.suggest_float('min_gain_to_split', 1e-6, 0.1, log = True),\n",
    "            'bagging_freq' : 1,\n",
    "            'bagging_fraction' : 0.95,\n",
    "            'feature_fraction' : 0.15,\n",
    "            'min_data_per_group' : trial.suggest_int('min_data_per_group', 1, 15),\n",
    "            'cat_smooth' : trial.suggest_float('cat_smooth', 0, 100.0),\n",
    "            'max_cat_threshold' : trial.suggest_int('max_cat_threshold', 1, 50),\n",
    "            'cat_l2' : trial.suggest_float('cat_l2', 1e-8, 10.0, log = True),\n",
    "            'max_cat_to_onehot' : trial.suggest_int('max_cat_to_onehot', 1, 10),\n",
    "            'n_jobs': 6,\n",
    "            'importance_type' : 'gain',\n",
    "            # 'random_state' : 137\n",
    "        }\n",
    "\n",
    "        callbacks = [lgbm.log_evaluation(1)]\n",
    "        if n_estimators is None:\n",
    "            callbacks.append(lgbm.early_stopping(250))\n",
    "        if evaluate:\n",
    "            lgbm_logger.info(f'Evaluation Trial')\n",
    "        else:\n",
    "            lgbm_logger.info(f'Trial number : {trial.number}')\n",
    "            callbacks.append(optuna.integration.LightGBMPruningCallback(trial, 'l2', 'valid_0'))\n",
    "        \n",
    "        regression_model = lgbm.LGBMRegressor(**regressor_params)\n",
    "\n",
    "        regression_model.fit(\n",
    "            X_t.to_numpy(),\n",
    "            y_train,\n",
    "            categorical_feature = cats,\n",
    "            eval_set = [(X_v.to_numpy(), y_val)],\n",
    "            eval_metric = 'l2',\n",
    "            feature_name = columns,\n",
    "            callbacks = callbacks\n",
    "        )\n",
    "\n",
    "        if evaluate:\n",
    "            print(f'Model : {index}')\n",
    "            print(f'Best iteration : {regression_model.best_iteration_}')\n",
    "            print(f\"Best validation RMSE : {regression_model.best_score_['valid_0']['l2']}\")\n",
    "            print(f'In-sample r^2 : {regression_model.score(X_t.to_numpy(), y_train)}')\n",
    "            print(f'Early stopping validation set r^2 : {regression_model.score(X_v.to_numpy(), y_val)}')\n",
    "            if test_set is not None:\n",
    "                test_data, test_target = test_set\n",
    "                print(f'Out-of-sample r^2 : {regression_model.score(pipe.fit_transform(test_data).to_numpy(), test_target)}')\n",
    "            print()\n",
    "            return (pipe, regression_model)\n",
    "\n",
    "        return regression_model.best_score_['valid_0']['l2']\n",
    "\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'likes'\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction = 'minimize',\n",
    "    study_name = f'tree_regressor_for_{target}',\n",
    "    storage = f\"sqlite:///TreeModels/optuna_study_for_{target}_none.db\",\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource = 200),\n",
    "    # sampler = optuna.samplers.TPESampler(),\n",
    "    load_if_exists = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDStreamTee:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __enter__(self):\n",
    "        sys.stdout.flush()\n",
    "        self.old_stdout = sys.stdout\n",
    "        self.file = open(self.filename, 'a')\n",
    "        sys.stdout = self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.flush()\n",
    "        self.file.close()\n",
    "        sys.stdout = self.old_stdout\n",
    "\n",
    "    def write(self, data):\n",
    "        self.file.write(data)\n",
    "        self.old_stdout.write(data)\n",
    "\n",
    "    def flush(self):\n",
    "        self.file.flush()\n",
    "        self.old_stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with STDStreamTee('FullModels/regressors/regression_scores') as tee:\n",
    "    for index in range(20, 30):\n",
    "        eval_trial = objective_factory(X_combined, likes_combined, X_combined, likes_combined, evaluate = True, test_set = (X_test, likes_test), n_estimators = 2450, index = index)\n",
    "        likes_pipe, likes_regressor = eval_trial(study.best_trial)\n",
    "        joblib.dump(likes_pipe, f'FullModels/regressors/likes_pipeline_index{index}.joblib')\n",
    "        likes_regressor.booster_.save_model(f'FullModels/regressors/lgbm_likes_index{index}.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'retweets'\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction = 'minimize',\n",
    "    study_name = f'tree_regressor_for_{target}',\n",
    "    storage = f\"sqlite:///TreeModels/optuna_study_for_{target}_none.db\",\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource = 200),\n",
    "    # sampler = optuna.samplers.TPESampler(),\n",
    "    load_if_exists = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trial = objective_factory(X_combined, retweets_combined, X_combined, retweets_combined, evaluate = True, test_set = (X_test, retweets_test), n_estimators = 1800, index = 29)\n",
    "retweets_pipe, retweets_regressor = eval_trial(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(retweets_pipe, f'FullModels/regressors/retweets_pipeline_index29.joblib')\n",
    "retweets_regressor.booster_.save_model(f'FullModels/regressors/lgbm_retweets_index29.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'replies'\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction = 'minimize',\n",
    "    study_name = f'tree_regressor_for_{target}',\n",
    "    storage = f\"sqlite:///TreeModels/optuna_study_for_{target}_none.db\",\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource = 200),\n",
    "    # sampler = optuna.samplers.TPESampler(),\n",
    "    load_if_exists = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trial = objective_factory(X_combined, replies_combined, X_combined, replies_combined, evaluate = True, test_set = (X_test, replies_test), n_estimators = 2250, index = 29)\n",
    "replies_pipe, replies_regressor = eval_trial(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(replies_pipe, f'FullModels/regressors/replies_pipeline_index29.joblib')\n",
    "replies_regressor.booster_.save_model(f'FullModels/regressors/lgbm_replies_index29.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full gradient-boosted tree ensemble\n",
    "\n",
    "Training the tree model on the full dataset (i.e. train, validation, and test). Evaluation metrics are unreliable since the test data is included in the training of the full model, but I expect the performance of the full model to be better than the tree models trained excluding the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'likes'\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction = 'minimize',\n",
    "    study_name = f'tree_regressor_for_{target}',\n",
    "    storage = f\"sqlite:///TreeModels/optuna_study_for_{target}_none.db\",\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource = 200),\n",
    "    # sampler = optuna.samplers.TPESampler(),\n",
    "    load_if_exists = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fulldata, likes_fulldata, retweets_fulldata, replies_fulldata = preprocess([*train, *val, *test], train_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trial = objective_factory(X_fulldata, likes_fulldata, X_fulldata, likes_fulldata, evaluate = True, test_set = (X_test, likes_test), n_estimators = 2450, index = 29)\n",
    "likes_pipe, likes_regressor = eval_trial(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(likes_pipe, f'FinalModels/likes_pipeline.joblib')\n",
    "likes_regressor.booster_.save_model(f'FinalModels/lgbm_likes.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'retweets'\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction = 'minimize',\n",
    "    study_name = f'tree_regressor_for_{target}',\n",
    "    storage = f\"sqlite:///TreeModels/optuna_study_for_{target}_none.db\",\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource = 200),\n",
    "    # sampler = optuna.samplers.TPESampler(),\n",
    "    load_if_exists = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trial = objective_factory(X_fulldata, retweets_fulldata, X_fulldata, retweets_fulldata, evaluate = True, test_set = (X_test, retweets_test), n_estimators = 1800, index = 29)\n",
    "retweets_pipe, retweets_regressor = eval_trial(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(retweets_pipe, f'FinalModels/retweets_pipeline.joblib')\n",
    "retweets_regressor.booster_.save_model(f'FinalModels/lgbm_retweets.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'replies'\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction = 'minimize',\n",
    "    study_name = f'tree_regressor_for_{target}',\n",
    "    storage = f\"sqlite:///TreeModels/optuna_study_for_{target}_none.db\",\n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner(min_resource = 200),\n",
    "    # sampler = optuna.samplers.TPESampler(),\n",
    "    load_if_exists = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_trial = objective_factory(X_fulldata, replies_fulldata, X_fulldata, replies_fulldata, evaluate = True, test_set = (X_test, replies_test), n_estimators = 2250, index = 29)\n",
    "replies_pipe, replies_regressor = eval_trial(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(replies_pipe, f'FinalModels/replies_pipeline.joblib')\n",
    "replies_regressor.booster_.save_model(f'FinalModels/lgbm_replies.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_pipeline = joblib.load('FinalModels/likes_pipeline.joblib')\n",
    "retweets_pipeline = joblib.load('FinalModels/retweets_pipeline.joblib')\n",
    "replies_pipeline = joblib.load('FinalModels/replies_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_section_encoder = likes_pipeline.steps[0][1]._ordinalencoder\n",
    "likes_section_encoder.feature_names_in_ = None\n",
    "joblib.dump(likes_section_encoder, 'FinalModels/likes_section_encoder.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_section_encoder = retweets_pipeline.steps[0][1]._ordinalencoder\n",
    "retweets_section_encoder.feature_names_in_ = None\n",
    "joblib.dump(retweets_section_encoder, 'FinalModels/retweets_section_encoder.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_section_encoder = replies_pipeline.steps[0][1]._ordinalencoder\n",
    "replies_section_encoder.feature_names_in_ = None\n",
    "joblib.dump(replies_section_encoder, 'FinalModels/replies_section_encoder.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_section_encoder = joblib.load('FinalModels/likes_section_encoder.joblib')\n",
    "retweets_section_encoder = joblib.load('FinalModels/retweets_section_encoder.joblib')\n",
    "replies_section_encoder = joblib.load('FinalModels/replies_section_encoder.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model and SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined_processed = likes_pipeline.transform(X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(sparse = False, handle_unknown = 'ignore'), ['section', 'dayofweek'])\n",
    "], remainder = 'passthrough')\n",
    "\n",
    "imputer = IterativeImputer()\n",
    "\n",
    "transform_pipe = Pipeline([\n",
    "    ('onehot', onehotencoder),\n",
    "    ('impute', imputer),\n",
    "])\n",
    "\n",
    "reg_pipe = Pipeline([\n",
    "    # ('poly', PolynomialFeatures()),\n",
    "    ('linear_model', SGDRegressor(penalty = 'l2'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined_processed = transform_pipe.fit_transform(X_combined_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # 'poly__degree': [2, 3, 4],\n",
    "    'linear_model__alpha': loguniform(1e-6, 100),\n",
    "    # 'linear_model__l1_ratio': loguniform(1e-6, 100)\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(reg_pipe, params, n_iter = 100, verbose = 3, n_jobs = None, cv = 3)\n",
    "rs.fit(X_combined_processed, likes_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_combined_processed, likes_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.score(X_combined_processed, likes_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.score(transform_pipe.transform(likes_pipeline.transform(X_test)), likes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pipe = Pipeline([\n",
    "    ('svm', SVR())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'svm__C': loguniform(1e-6, 100),\n",
    "    'svm__epsilon': loguniform(1e-6, 100),\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(reg_pipe, params, n_iter = 1000, verbose = 3, n_jobs = 6, cv = 3)\n",
    "rs.fit(X_combined_processed, likes_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances\n",
    "\n",
    "Note that feature importances may not be so reliable if features are strongly collinear or mutually related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_regressor = lgbm.Booster(model_file = 'FinalModels/lgbm_likes.model')\n",
    "retweets_regressor = lgbm.Booster(model_file = 'FinalModels/lgbm_retweets.model')\n",
    "replies_regressor = lgbm.Booster(model_file = 'FinalModels/lgbm_replies.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_pipeline = joblib.load('FinalModels/likes_pipeline.joblib')\n",
    "retweets_pipeline = joblib.load('FinalModels/retweets_pipeline.joblib')\n",
    "replies_pipeline = joblib.load('FinalModels/replies_pipeline.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_evaluation(importance_type, regressor):\n",
    "\n",
    "    with open('ProcessedData/topic_words_short.json', 'rt') as f:\n",
    "        topic_words = json.load(f)\n",
    "\n",
    "    importances = []\n",
    "    for index, importance_score in sorted(enumerate(regressor.feature_importance(importance_type)), key = lambda x: -x[1]):\n",
    "        importance_tuple = (regressor.feature_name()[index], importance_score)\n",
    "        if 'topic_' == importance_tuple[0][:6]:\n",
    "            topic_num = int(importance_tuple[0][6:])\n",
    "            importance_tuple = (topic_words[topic_num][1], importance_tuple[1])\n",
    "        importances.append(importance_tuple)\n",
    "\n",
    "    return importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_evaluation('gain', likes_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_evaluation('split', likes_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_evaluation('gain', retweets_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_evaluation('split', retweets_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_evaluation('gain', replies_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_evaluation('split', replies_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_regressor.params['objective'] = 'regression'\n",
    "explainer = shap.TreeExplainer(likes_regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fulldata, likes_fulldata, retweets_fulldata, replies_fulldata = preprocess([*train, *val, *test], train_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(likes_pipeline.transform(X_fulldata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "mpl.rcParams['axes.facecolor'] = 'white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, likes_pipeline.transform(X_fulldata), max_display = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, likes_pipeline.transform(X_fulldata), max_display = 1000, plot_type = 'bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52dc165078b550922a92e20afc6187e50fa6838252eb052b89f3983eda27ca00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
