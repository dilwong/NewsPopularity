{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping\n",
    "\n",
    "## Scraping Twitter data\n",
    "\n",
    "Here I scrape tweets from @nytimes made between 2022-01-01 and mid-April 2022 using the Twitter API, twint, and snscrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Getting tweets from @nytimes using the Twitter API v2.\n",
    "    Unfortunately, I was only able to get 30 days of tweets using this method.\n",
    "'''\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "twitterAPI = 'https://api.twitter.com/2/users/'\n",
    "nytUserID = '807095' # Twitter ID for the New York Times\n",
    "startTime = '2022-01-01T00:00:00.000Z' # Beginning of the year\n",
    "endTime = '2022-04-30T00:00:00.000Z' # End of April\n",
    "\n",
    "resultingJSON = {'meta': {'next_token': ''}}\n",
    "while True:\n",
    "    try:\n",
    "        pagination_token = resultingJSON['meta']['next_token']\n",
    "        if pagination_token != '':\n",
    "            pagination_token = 'pagination_token=' + pagination_token\n",
    "    except KeyError:\n",
    "        break\n",
    "    fields = ('/tweets?max_results=100&' +\n",
    "                pagination_token +\n",
    "                '&start_time=' + startTime +\n",
    "                '&end_time=' + endTime +\n",
    "                '&tweet.fields=id,created_at,text,author_id,in_reply_to_user_id,referenced_tweets,attachments,geo,entities,public_metrics,source,context_annotations,conversation_id&media.fields=media_key,duration_ms,height,preview_image_url,type,url,width,public_metrics,alt_text'\n",
    "    )\n",
    "    URL = twitterAPI + nytUserID + fields\n",
    "\n",
    "    req = requests.get(URL, headers = {'Authorization': f'Bearer {os.environ[\"BEARER_TOKEN\"]}'})\n",
    "    resultingJSON = req.json()\n",
    "    \n",
    "    with open('nyt_' + pagination_token + '.json', 'w') as file:\n",
    "        json.dump(resultingJSON, file)\n",
    "\n",
    "    time.sleep(10) # To keep the number of requests well below the Twitter rate limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Getting tweets from @nytimes using twint.\n",
    "    Unfortunately, I was only able to get 10 days of tweets using this method.  Moreover, retweets are missing.\n",
    "'''\n",
    "\n",
    "import twint\n",
    "config = twint.Config()\n",
    "config.Username = 'nytimes'\n",
    "config.Since = '2022-01-01'\n",
    "config.Store_json = True\n",
    "config.Output = 'nyt_data_twint.json'\n",
    "config.Retweets = True\n",
    "\n",
    "twint.run.Search(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Getting tweets from @nytimes using snscrape, on command line.\n",
    "    This method seems to be able to get tweets from Jan 2022, unlike the Twitter API and twint.  However, retweets are missing.\n",
    "'''\n",
    "\n",
    "# snscrape --jsonl --progress --since 2022-01-01 twitter-search \"from:nytimes until:2022-04-30\" >> nyt_twitter_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Twitter data into a postgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "\n",
    "conn = psycopg2.connect(host = 'localhost')\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create database\n",
    "cursor.execute('CREATE DATABASE nytpopular;')\n",
    "\n",
    "conn.close()\n",
    "cursor.close()\n",
    "\n",
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tweets table\n",
    "cursor.execute('''CREATE TABLE tweets (\n",
    "\tid BIGINT PRIMARY KEY,\n",
    "    retweet_count INT,\n",
    "    reply_count INT,\n",
    "    like_count INT,\n",
    "    quote_count INT,\n",
    "    url VARCHAR(255),\n",
    "    text TEXT,\n",
    "    date TIMESTAMPTZ,\n",
    "    video BOOLEAN,\n",
    "    photo BOOLEAN\n",
    ");''')\n",
    "\n",
    "def insertTweetintoDB(jsonObject, cursor):\n",
    "    fields = OrderedDict()\n",
    "    fields['id'] = jsonObject['id']\n",
    "    fields['retweet_count'] = jsonObject['retweetCount']\n",
    "    fields['reply_count'] = jsonObject['replyCount']\n",
    "    fields['like_count'] = jsonObject['likeCount']\n",
    "    fields['quote_count'] = jsonObject['quoteCount']\n",
    "    if jsonObject['outlinks'] is None:\n",
    "        return\n",
    "    fields['url'] = ' '.join(jsonObject['outlinks'])\n",
    "    fields['text'] = jsonObject['content']\n",
    "    fields['date'] = jsonObject['date'].replace('T',' ')\n",
    "    if ('media' in jsonObject) and (jsonObject['media'] is not None):\n",
    "        containsPhoto = any(['Photo' in elem['_type'] for elem in jsonObject['media']])\n",
    "        containsVideo = any(['Video' in elem['_type'] for elem in jsonObject['media']])\n",
    "    else:\n",
    "        containsPhoto = False\n",
    "        containsVideo = False\n",
    "    fields['video'] = 'TRUE' if containsVideo else 'FALSE'\n",
    "    fields['photo'] = 'TRUE' if containsPhoto else 'FALSE'\n",
    "    keys, items = zip(*fields.items())\n",
    "    keys = sql.SQL(',').join(map(sql.Identifier, keys))\n",
    "    items = sql.Literal(items)\n",
    "    command = sql.SQL('INSERT INTO tweets ({}) VALUES {} ON CONFLICT (id) DO NOTHING;').format(keys, items)\n",
    "    cursor.execute(command)\n",
    "\n",
    "with open('ScrappedData/snscrape/nyt_twitter_data.json','r') as f:\n",
    "    for line in f:\n",
    "        insertTweetintoDB(json.loads(line), cursor)\n",
    "\n",
    "cursor.execute(\"DELETE FROM tweets WHERE text ~ '^[^a-zA-z0-9\\u201c\\u201d\\\"@#]'\") # Drop some foreign language tweets from the table.\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping New York Times articles using Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a context manager for graceful stopping a program when hitting CTRL + C.\n",
    "# I am defining this so that I can pause scraping whenever I want.\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import signal\n",
    "\n",
    "class Interrupt:\n",
    "  def __init__(self):\n",
    "    self.interrupt = False\n",
    "  \n",
    "def signal_handler(signal, frame):\n",
    "  interrupt.interrupt = True\n",
    "  print('Terminating...')\n",
    "\n",
    "@contextmanager\n",
    "def graceful_exit():\n",
    "  original_sigint_handler = signal.getsignal(signal.SIGINT)\n",
    "  interrupt = Interrupt()\n",
    "  signal.signal(signal.SIGINT, signal_handler)\n",
    "  try:\n",
    "    yield interrupt\n",
    "  except:\n",
    "    raise\n",
    "  finally:\n",
    "    signal.signal(signal.SIGINT, original_sigint_handler)\n",
    "\n",
    "# Get links to New York Times articles for all tweets.\n",
    "import psycopg2\n",
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute('SELECT id, url FROM tweets;')\n",
    "    allURLs = cursor.fetchall()\n",
    "conn.close()\n",
    "urlStack = list(reversed(allURLs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch FireFox with Selenium\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "firefoxProfile = '/Users/dilwong/Library/Application Support/Firefox/Profiles/aiyantvn.selenium' # This is my FireFox profile for selenium\n",
    "driver = webdriver.Firefox(firefox_profile=firefoxProfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the NYT articles and save (1) the HTML and (2) a JSON file containing the title, summary paragraph, article text, full URL, # of comments, and whether the page has audio/video\n",
    "\n",
    "with graceful_exit() as interrupt: # Note that CTRL + C will also close FireFox...\n",
    "    while True:\n",
    "        try:\n",
    "            id_, url = urlStack.pop()\n",
    "        except IndexError:\n",
    "            print('DONE!!!')\n",
    "            break\n",
    "        driver.get(url)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # Scroll to bottom to load entire page\n",
    "        fullHTML = driver.page_source\n",
    "        with open(f'ScrappedData/nytArticles/fullHTML/{id_}.html', 'w') as file:\n",
    "            file.write(fullHTML)\n",
    "        fullURL = driver.current_url\n",
    "        parsedURL = urlparse(fullURL)\n",
    "        if 'nytimes.com' not in parsedURL.hostname: # If the link is not a NYT article, skip it.\n",
    "            continue\n",
    "        if 'https://www.nytimes.com/interactive/2021/us/covid-cases.html' in fullURL: # If the link is the COVID case graph, skip it.\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        if 'https://www.nytimes.com/interactive/2022/upshot/wordle-bot.html' in fullURL: # Skip Wordle\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        if 'https://www.nytimes.com/puzzles/spelling-bee' in fullURL: # Skip Spelling Bee\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        if 'https://www.nytimes.com/newsletters/the-daily' in fullURL: # Skip The Daily\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "        soup = BeautifulSoup(fullHTML)\n",
    "        if parsedURL.path.split('/')[1] == 'live':\n",
    "            liveSection = soup.find_all(attrs={'data-url': re.compile(parsedURL.fragment)})\n",
    "            if liveSection:\n",
    "                titleText = ' '.join([elem.text for elem in liveSection[0].find_all(class_='css-16ra6z9')])\n",
    "                summaryText = None\n",
    "                articleText = ' '.join([elem.text for elem in liveSection[0].find_all(class_='evys1bk0')])\n",
    "                hasVideo = bool(liveSection[0].find_all('video'))\n",
    "                hasAudio = bool(liveSection[0].find_all('audio'))\n",
    "            else:\n",
    "                #titleText = ' '.join([elem.text for elem in soup.find_all(class_='e1h9rw200')])\n",
    "                #summaryText = ' '.join([elem.text for elem in soup.select('.css-54b9nt.evys1bk0')])\n",
    "                #articleText = ' '.join([elem.text for elem in soup.select('.live-blog-post-content.evys1bk0')])\n",
    "                continue\n",
    "        else:\n",
    "            titleText = ' '.join([elem.text for elem in soup.find_all(class_='e1h9rw200')])\n",
    "            if titleText == '':\n",
    "                titleText = ' '.join([elem.text for elem in soup.find_all(class_='edye5kn2')])\n",
    "            if titleText == '':\n",
    "                titleText = ' '.join([elem.text for elem in soup.find_all(class_='_7503bcc6')])\n",
    "            if titleText == '':\n",
    "                titleText = ' '.join([elem.text for elem in soup.select('h1[data-scp=\"title hidden\"]')])\n",
    "            if titleText == '':\n",
    "                titleText = ' '.join([elem.text for elem in soup.select('h1[itemprop=\"headline\"]')])\n",
    "            if titleText == '':\n",
    "                titleText = ' '.join([elem.text for elem in soup.select('header.title-header > h1')])\n",
    "            summaryText = ' '.join([elem.text for elem in soup.find_all(class_='e1wiw3jv0')])\n",
    "            if summaryText == '':\n",
    "                summaryText = ' '.join([elem.text for elem in soup.find_all(class_='edye5kn3')])\n",
    "            if summaryText == '':\n",
    "                summaryText = ' '.join([elem.text for elem in soup.select('header.title-header > p.g-dek')])\n",
    "            if summaryText == '':\n",
    "                summaryText = None\n",
    "            articleText = (' '.join([elem.text for elem in soup.find_all(class_='g-body')]).replace('\\n','').replace('\\t','')\n",
    "                            + ' ' + ' '.join([elem.text for elem in soup.find_all(class_='evys1bk0')])).strip()\n",
    "            if articleText == '':\n",
    "                articleText = (' '.join([elem.text for elem in soup.select(\"[data-testid='lede'] > p\")])\n",
    "                            + ' ' + ' '.join([elem.text for elem in soup.select(\"[data-gtm-element='intro'] > p\")])\n",
    "                            + ' ' + ' '.join([elem.text for elem in soup.select(\"div[role='tabpanel'] p\")])).strip()\n",
    "            if articleText == '':\n",
    "                articleText = ' '.join([elem.text for elem in soup.select('div.blurb')]).replace('\\n', ' ')\n",
    "            if articleText == '':\n",
    "                articleText = ' '.join([elem.text for elem in soup.select('p.css-fytyl0')])\n",
    "            articleText = articleText.replace('\\xa0',' ')\n",
    "            hasVideo = bool(soup.find_all('video'))\n",
    "            hasAudio = bool(soup.find_all('audio'))\n",
    "        if parsedURL.path.split('/')[1] == 'recipes':\n",
    "            titleText = soup.select('.recipe-title')[0].text.strip()\n",
    "            articleText = ' '.join([elem.text for elem in soup.select('.topnote > p')]) + ' ' + ' '.join([elem.text for elem in soup.select('ol.recipe-steps')]).replace('\\n',' ')\n",
    "        if (titleText == '') or (articleText == ''):\n",
    "            if parsedURL.path.split('/')[1] == 'video': # For video-only articles\n",
    "                if articleText == '':\n",
    "                    articleText = None\n",
    "                if titleText == '':\n",
    "                    titleText = None\n",
    "            elif parsedURL.path.split('/')[1] == 'interactive':\n",
    "                try:\n",
    "                    articleText = (' '.join([elem.text for elem in soup.select('p.css-1xh7jop')]) + ' ' + ' '.join([elem.text for elem in soup.select('p.css-1jeom2t')])).strip()\n",
    "                    possibleTitle = soup.select('h2.css-6fr66q')\n",
    "                    if possibleTitle:\n",
    "                        titleText = possibleTitle[0].text\n",
    "                    possibleTitle = soup.select('h2.css-1ifcf0b')\n",
    "                    if possibleTitle:\n",
    "                        titleText = possibleTitle[0].text\n",
    "                except:\n",
    "                    print(f'Stopped at {id_}, {url} because either titleText or articleText is missing.')\n",
    "                    break\n",
    "                if (titleText == '') or (articleText == ''):\n",
    "                    print(f'Stopped at {id_}, {url} because either titleText or articleText is missing.')\n",
    "                    break\n",
    "            else:\n",
    "                print(f'Stopped at {id_}, {url} because either titleText or articleText is missing.')\n",
    "                break\n",
    "        nComment = soup.find(class_ = 'css-1dtr3u3') # Are there comments?\n",
    "        if nComment is not None:\n",
    "            nComment = nComment.text\n",
    "            if nComment == '+':\n",
    "                nComment = 0\n",
    "            else:\n",
    "                if nComment[-1] == 'k': # If there are comments, how many?\n",
    "                    nComment = int(float(nComment[:-1]) * 1000)\n",
    "                else:\n",
    "                    nComment = int(nComment)\n",
    "        try:\n",
    "            nComment_alt = soup.select('button#comments-speech-bubble-bigBottom')[0].text.split(' ')[1] # More accurate comment count\n",
    "            if nComment_alt == '+':\n",
    "                nComment = 0\n",
    "            else:\n",
    "                nComment = int(nComment_alt)\n",
    "        except:\n",
    "            pass\n",
    "        if nComment is None:\n",
    "            try:\n",
    "                switched_iframe = False\n",
    "                frame = driver.find_elements(By.CSS_SELECTOR, 'div#disqus_thread > iframe[src*=\"https://disqus.com/embed/comments/\"]')[0]\n",
    "                driver.switch_to.frame(frame)\n",
    "                switched_iframe = True\n",
    "                newsoup = BeautifulSoup(driver.page_source)\n",
    "                commentCount = newsoup.find(class_='comment-count').text.split()\n",
    "                if commentCount[1] == 'comments':\n",
    "                    nComment = int(commentCount[0])\n",
    "            except:\n",
    "                pass\n",
    "            finally:\n",
    "                nComment = None\n",
    "                if switched_iframe:\n",
    "                    driver.switch_to.parent_frame()\n",
    "        if nComment is None:\n",
    "            try:\n",
    "                allTab = soup.select('#all-tab > .nytc---notessection---threadCount')\n",
    "                if allTab:\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    time.sleep(2)\n",
    "                    fullHTML = driver.page_source\n",
    "                    soup = BeautifulSoup(fullHTML)\n",
    "                    allTab = soup.select('#all-tab > .nytc---notessection---threadCount')\n",
    "                    nComment = int(allTab[0].text)\n",
    "            except:\n",
    "                pass\n",
    "        jsonObject = {\n",
    "                        'title': titleText,\n",
    "                        'summary': summaryText,\n",
    "                        'article': articleText,\n",
    "                        'full_url': fullURL,\n",
    "                        'comments': nComment,\n",
    "                        'has_video': hasVideo,\n",
    "                        'has_audio': hasAudio\n",
    "        }\n",
    "        with open(f'ScrappedData/nytArticles/articleText/{id_}.json', 'w') as file:\n",
    "            json.dump(jsonObject, file)\n",
    "        time.sleep(random.randrange(5,15)) # Wait 5-15 seconds before fetching next article. People probably don't like their websites being scraped, so this is out of politeness.\n",
    "        if interrupt.interrupt:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the scrapped NYT articles into a postgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import os\n",
    "\n",
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tweets table\n",
    "cursor.execute('''CREATE TABLE articles (\n",
    "\tid BIGINT PRIMARY KEY,\n",
    "    title VARCHAR(255),\n",
    "    summary TEXT,\n",
    "    article TEXT,\n",
    "    url VARCHAR(500),\n",
    "    comments INT,\n",
    "    video BOOLEAN,\n",
    "    audio BOOLEAN\n",
    ");''')\n",
    "\n",
    "def insertArticleintoDB(jsonObject, cursor):\n",
    "    fields = OrderedDict()\n",
    "    fields['id'] = jsonObject['id']\n",
    "    fields['title'] = jsonObject['title']\n",
    "    fields['summary'] = jsonObject['summary']\n",
    "    fields['article'] = jsonObject['article']\n",
    "    fields['url'] = jsonObject['full_url']\n",
    "    fields['comments'] = jsonObject['comments']\n",
    "    fields['video'] = 'TRUE' if jsonObject['has_video'] else 'FALSE'\n",
    "    fields['audio'] = 'TRUE' if jsonObject['has_audio'] else 'FALSE'\n",
    "    keys, items = zip(*fields.items())\n",
    "    keys = sql.SQL(',').join(map(sql.Identifier, keys))\n",
    "    items = sql.Literal(items)\n",
    "    command = sql.SQL('INSERT INTO articles ({}) VALUES {} ON CONFLICT (id) DO NOTHING;').format(keys, items)\n",
    "    cursor.execute(command)\n",
    "\n",
    "articleFilenames = os.listdir('ScrappedData/nytArticles/articleText/')\n",
    "\n",
    "for filename in articleFilenames:\n",
    "    if '.json' in filename:\n",
    "        with open('ScrappedData/nytArticles/articleText/' + filename) as f:\n",
    "            jsonObject = json.load(f)\n",
    "            jsonObject['id'] = int(filename.split('.')[0])\n",
    "            insertArticleintoDB(jsonObject, cursor)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case, purge any articles that don't have an associated tweet. This shouldn't happen.\n",
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "conn.autocommit = True\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute('''DELETE FROM articles\n",
    "                    WHERE id IN (SELECT articles.id AS a_id FROM tweets RIGHT JOIN articles ON tweets.id = articles.id WHERE tweets.id is NULL);''')\n",
    "conn.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get article metadata through the New York Times API\n",
    "\n",
    "The New York Times API has some interesting metadata, including keywords for each article.<br>\n",
    "If one wanted, one could also get vocabulary/tags from the Semantic API and user comments from the Community API.<br>\n",
    "Here, I only fetch information from the Archive API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "\n",
    "apiKey = '' # Replace with NYT API key\n",
    "months = [12, 1, 2, 3, 4]\n",
    "years = [2019] + [2022] * 4\n",
    "\n",
    "for month, year in zip(months, years):\n",
    "    req = requests.get(f'https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={apiKey}')\n",
    "    with open(f'nyt_metadata_{month:02}_{year}.json', 'w', encoding = 'utf-8') as f:\n",
    "        json.dump(req.json(), f, indent = 4)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the metadata into POSTGRES database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import os\n",
    "\n",
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''CREATE TABLE metadata (\n",
    "\tid VARCHAR(255) PRIMARY KEY,\n",
    "    abstract TEXT,\n",
    "    url VARCHAR(255),\n",
    "    lead_paragraph TEXT,\n",
    "    title VARCHAR(255),\n",
    "    date TIMESTAMPTZ,\n",
    "    document_type VARCHAR(100),\n",
    "    news_desk VARCHAR(100),\n",
    "    section VARCHAR(100),\n",
    "    subsection VARCHAR(100),\n",
    "    word_count SMALLINT,\n",
    "    keywords TEXT\n",
    ");''')\n",
    "\n",
    "def insertMetadataintoDB(jsonObject, cursor):\n",
    "    fields = OrderedDict()\n",
    "    fields['id'] = jsonObject['_id']\n",
    "    fields['abstract'] = jsonObject['abstract']\n",
    "    fields['url'] = jsonObject['web_url']\n",
    "    fields['lead_paragraph'] = jsonObject['lead_paragraph']\n",
    "    fields['title'] = jsonObject['headline']['main']\n",
    "    fields['date'] = jsonObject['pub_date'].replace('T',' ')\n",
    "    fields['document_type'] = jsonObject['document_type']\n",
    "    fields['news_desk'] = jsonObject['news_desk']\n",
    "    fields['section'] = jsonObject['section_name']\n",
    "    fields['subsection'] = jsonObject['subsection_name'] if 'subsection_name' in jsonObject else None\n",
    "    fields['word_count'] = jsonObject['word_count']\n",
    "    fields['keywords'] = '|'.join([entry['value'] for entry in jsonObject['keywords']]) or None\n",
    "    keys, items = zip(*fields.items())\n",
    "    keys = sql.SQL(',').join(map(sql.Identifier, keys))\n",
    "    items = sql.Literal(items)\n",
    "    command = sql.SQL('INSERT INTO metadata ({}) VALUES {} ON CONFLICT (id) DO NOTHING;').format(keys, items)\n",
    "    cursor.execute(command)\n",
    "\n",
    "articleFilenames = os.listdir('ScrappedData/nytMetadata/')\n",
    "\n",
    "for filename in articleFilenames:\n",
    "    if '.json' in filename:\n",
    "        with open('ScrappedData/nytMetadata/' + filename, 'r') as f:\n",
    "            wholeObject = json.load(f)\n",
    "            for jsonObject in wholeObject['response']['docs']:\n",
    "                insertMetadataintoDB(jsonObject, cursor)\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a table that maps tweet IDs (from Twitter) to NYT URIs (from the metadata).\n",
    "\n",
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "sqlStatement = '''CREATE TABLE id_map AS\n",
    "                SELECT articles.id tweet_id, metadata.id uri_id, articles.url article_url, metadata.url metadata_url FROM articles\n",
    "                LEFT JOIN metadata ON articles.url LIKE metadata.url || '%';\n",
    "                '''\n",
    "cursor.execute(sqlStatement)\n",
    "cursor.execute('ALTER TABLE id_map ADD PRIMARY KEY (tweet_id);')\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d567c0da7bb219f9727e7c3372c0847942a9fee9e17808e2da3a0a5b2750b84"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('python3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
