{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYT Article Text to OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import tiktoken\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from typing import Union, List\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import product\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import gzip\n",
    "\n",
    "import openai\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_not_exception_type\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "EMBEDDING_ENCODING = \"cl100k_base\"\n",
    "MAX_TOKENS = 8191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6), retry=retry_if_not_exception_type(openai.InvalidRequestError))\n",
    "def fetch_embedding(input_: Union[str, List[int]]) -> List[float]:\n",
    "    return openai.Embedding.create(input=input_, model=EMBEDDING_MODEL)[\"data\"][0][\"embedding\"]\n",
    "\n",
    "encoding = tiktoken.get_encoding(EMBEDDING_ENCODING)\n",
    "def fetch_encoding(text: str, truncate: bool = False) -> List[int]:\n",
    "    enc = encoding.encode(text)\n",
    "    if truncate:\n",
    "        enc = enc[:MAX_TOKENS]\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('ScrappedData/nytArticles/articleText/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "for filename in all_files:\n",
    "    with open(filename, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        json_data['id'] = os.path.splitext(os.path.basename(filename))[0]\n",
    "        if json_data['article'] is not None:\n",
    "            json_data['article'] = json_data['article'].replace(\"\\n\", \" \")\n",
    "            all_articles.append(json_data)\n",
    "all_articles_df = pd.DataFrame(all_articles)\n",
    "all_articles_df.index = all_articles_df['id']\n",
    "all_articles_df = all_articles_df.drop(['summary', 'id'], axis=1)\n",
    "article_text = all_articles_df['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles_df[\"n_tokens\"] = all_articles_df['article'].apply(lambda x: len(fetch_encoding(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: OpenAI sometimes randomly returns [nan] as the embedding. Just redo that entry.\n",
    "\n",
    "all_articles_df[\"embedding\"] = all_articles_df['article'].apply(\n",
    "    lambda x: fetch_embedding(fetch_encoding(x, truncate=True))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_articles_df = all_articles_df.drop('article', axis=1)\n",
    "all_articles_df.to_csv(\"EmbeddingData/articles_with_embeddings.csv\")\n",
    "all_articles_df.to_parquet('EmbeddingData/articles_with_embeddings.parquet.gzip', compression='gzip')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Euclidean Distances for Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df = pd.DataFrame(all_articles_df['embedding'].to_list())\n",
    "embeddings_distance_matrix = cdist(vector_df, vector_df, metric='euclidean')\n",
    "np.save('EmbeddingData/embeddings_distance_matrix.npy', embeddings_distance_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Distance Measure Based on gzip Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gzip_distance(text1: str, text2: str) -> float:\n",
    "    '''\n",
    "    Information-theory-based distance between two string using gzip.\n",
    "    Based on https://arxiv.org/abs/2212.09410\n",
    "    Not a symmetric distance! Also, distance of a text to itself is not zero!\n",
    "    '''\n",
    "    text1_compressed_length = len(gzip.compress(text1.encode()))\n",
    "    text2_compressed_length = len(gzip.compress(text2.encode()))  \n",
    "    combinedtext_compressed_length = len(gzip.compress(f'{text1} {text2}'.encode()))\n",
    "    return (combinedtext_compressed_length - min(text1_compressed_length, text2_compressed_length)) / max(text1_compressed_length, text2_compressed_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = article_text.to_list()\n",
    "# gzip_distance_matrix = np.array([[gzip_distance(text1, text2) for text1 in articles] for text2 in articles])\n",
    "gzip_distance_array = Parallel(n_jobs=-1)(delayed(gzip_distance)(text1, text2) for text1, text2 in product(articles, articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gzip_distance_matrix = np.array(gzip_distance_array).reshape(len(articles), len(articles))\n",
    "np.save('EmbeddingData/gzip_distance_matrix.npy', gzip_distance_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
