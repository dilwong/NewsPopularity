{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import Optional, Callable, List, Tuple\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.special import softmax\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tweet text, article title, article summary, and article sentences\n",
    "\n",
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute('''SELECT wordbags.id, tweets.text, articles.title, articles.summary, wordbags.sentences\n",
    "                        FROM wordbags\n",
    "                        LEFT JOIN articles ON wordbags.id = articles.id\n",
    "                        LEFT JOIN tweets ON wordbags.id = tweets.id;''')\n",
    "    texts = cursor.fetchall()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateFieldsinDB(fields, table_name, cursor):\n",
    "    keys, items = zip(*fields.items())\n",
    "    keys = sql.SQL(',').join(map(sql.Identifier, keys))\n",
    "    items = sql.Literal(items)\n",
    "    table_name = sql.Identifier(table_name)\n",
    "    key_item_pairs = sql.SQL(', ').join(\n",
    "        sql.Composed([\n",
    "            sql.Identifier(k), sql.SQL(' = '), sql.Literal(v)\n",
    "        ]) for k, v in fields.items()\n",
    "    )\n",
    "    command = sql.SQL(\"\"\"INSERT INTO {} ({})\n",
    "        VALUES {}\n",
    "        ON CONFLICT (id) DO UPDATE\n",
    "        SET {};\"\"\").format(table_name, keys, items, key_item_pairs)\n",
    "    cursor.execute(command)\n",
    "\n",
    "\n",
    "def remove_http(text):\n",
    "    return ' '.join([word for word in text.split() if not word.startswith('http')])\n",
    "\n",
    "\n",
    "def average_dictionary(dicts : List[dict]) -> dict:\n",
    "    avg_dict = dict()\n",
    "    for key in dicts[0]:\n",
    "        avg_dict[key] = sum(d[key] for d in dicts)/len(dicts)\n",
    "    return avg_dict\n",
    "\n",
    "\n",
    "def sentiment_processing(predict : Callable, filter_condition : Optional[Callable] = None) -> Tuple[List, List, List, List, List]:\n",
    "    ids = []\n",
    "    tweet_text_scores = []\n",
    "    article_title_scores = []\n",
    "    article_summary_scores = []\n",
    "    article_main_scores = []\n",
    "    for id, tweet_text, article_title, article_summary, article_sentences in tqdm(texts):\n",
    "        ids.append(str(id))\n",
    "        if tweet_text is None:\n",
    "            tweet_text_scores.append(None)\n",
    "        else:\n",
    "            tweet_text_scores.append(predict(remove_http(tweet_text)))\n",
    "        if article_title is None:\n",
    "            article_title_scores.append(None)\n",
    "        else:\n",
    "            article_title_scores.append(predict(article_title))\n",
    "        if article_summary is None:\n",
    "            article_summary_scores.append(None)\n",
    "        else:\n",
    "            article_summary_scores.append(predict(article_summary))\n",
    "        if filter_condition is None:\n",
    "            filter_condition = lambda x: True\n",
    "        sentence_scores = [entry for entry in (predict(sent) for sent in article_sentences) if filter_condition(entry)]\n",
    "        if len(sentence_scores) == 0:\n",
    "            article_main_scores.append(predict(' '.join(article_sentences)))\n",
    "        else:\n",
    "            article_main_scores.append(average_dictionary(sentence_scores))\n",
    "    return ids, tweet_text_scores, article_title_scores, article_summary_scores, article_main_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.float32):\n",
    "            return float(obj)\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VADER\n",
    "\n",
    "Rule-based sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_ids, vader_tweet_texts, vader_article_titles, vader_article_summaries, vader_article_main = sentiment_processing(analyzer.polarity_scores, lambda entry: entry['neu'] != 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_sentiment_data = dict(\n",
    "    vader_ids = vader_ids,\n",
    "    vader_tweet_texts = vader_tweet_texts,\n",
    "    vader_article_titles = vader_article_titles,\n",
    "    vader_article_summaries = vader_article_summaries,\n",
    "    vader_article_main = vader_article_main\n",
    ")\n",
    "\n",
    "with open('SentimentData/vader_sentiment_data.json', 'w') as f:\n",
    "    json.dump(vader_sentiment_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "Transformer-based sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentModel:\n",
    "\n",
    "    def __init__(self, model, max_length = None):\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "        self.config = AutoConfig.from_pretrained(model)\n",
    "        self.classifier = AutoModelForSequenceClassification.from_pretrained(model).to(self.device)\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    # Would probably be more efficient to batch data rather than send one at a time...\n",
    "    def predict(self, text):\n",
    "        with torch.no_grad():\n",
    "            tokenized_input = self.tokenizer(text, return_tensors = 'pt', truncation = True, max_length = self.max_length).to(self.device)\n",
    "            scores = softmax(self.classifier(**tokenized_input)[0][0].cpu().numpy()) # Don't need to detach when in torch.no_grad context.\n",
    "        return {label: scores[idx] for idx, label in self.config.id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DistilBERT from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert = SentimentModel('distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_ids, distilbert_tweet_texts, distilbert_article_titles, distilbert_article_summaries, distilbert_article_main = sentiment_processing(distilbert.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_sentiment_data = dict(\n",
    "    distilbert_ids = distilbert_ids,\n",
    "    distilbert_tweet_texts = distilbert_tweet_texts,\n",
    "    distilbert_article_titles = distilbert_article_titles,\n",
    "    distilbert_article_summaries = distilbert_article_summaries,\n",
    "    distilbert_article_main = distilbert_article_main\n",
    ")\n",
    "\n",
    "with open('SentimentData/distilbert_sentiment_data.json', 'w') as f:\n",
    "    json.dump(distilbert_sentiment_data, f, cls = NumpyEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa trained on ~124M tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = SentimentModel(r'cardiffnlp/twitter-roberta-base-sentiment-latest', max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_ids, roberta_tweet_texts, roberta_article_titles, roberta_article_summaries, roberta_article_main = sentiment_processing(roberta.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_sentiment_data = dict(\n",
    "    roberta_ids = roberta_ids,\n",
    "    roberta_tweet_texts = roberta_tweet_texts,\n",
    "    roberta_article_titles = roberta_article_titles,\n",
    "    roberta_article_summaries = roberta_article_summaries,\n",
    "    roberta_article_main = roberta_article_main\n",
    ")\n",
    "\n",
    "with open('SentimentData/roberta_sentiment_data.json', 'w') as f:\n",
    "    json.dump(roberta_sentiment_data, f, cls = NumpyEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SiEBERT - Sentiment RoBERTa trained on diverse English-language corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siebert = SentimentModel(r'siebert/sentiment-roberta-large-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siebert_ids, siebert_tweet_texts, siebert_article_titles, siebert_article_summaries, siebert_article_main = sentiment_processing(siebert.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siebert_sentiment_data = dict(\n",
    "    siebert_ids = siebert_ids,\n",
    "    siebert_tweet_texts = siebert_tweet_texts,\n",
    "    siebert_article_titles = siebert_article_titles,\n",
    "    siebert_article_summaries = siebert_article_summaries,\n",
    "    siebert_article_main = siebert_article_main\n",
    ")\n",
    "\n",
    "with open('SentimentData/siebert_sentiment_data.json', 'w') as f:\n",
    "    json.dump(siebert_sentiment_data, f, cls = NumpyEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing in an SQL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SentimentData/vader_sentiment_data.json', 'r') as f:\n",
    "    vader_sentiment_data = json.load(f)\n",
    "\n",
    "with open('SentimentData/distilbert_sentiment_data.json', 'r') as f:\n",
    "    distilbert_sentiment_data = json.load(f)\n",
    "\n",
    "with open('SentimentData/roberta_sentiment_data.json', 'r') as f:\n",
    "    roberta_sentiment_data = json.load(f)\n",
    "\n",
    "with open('SentimentData/siebert_sentiment_data.json', 'r') as f:\n",
    "    siebert_sentiment_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''CREATE TABLE sentiment (\n",
    "\n",
    "\tid BIGINT PRIMARY KEY,\n",
    "\n",
    "    vader_tweet_texts_neg REAL,\n",
    "    vader_tweet_texts_neu REAL,\n",
    "    vader_tweet_texts_pos REAL,\n",
    "    vader_tweet_texts_compound REAL,\n",
    "    vader_article_titles_neg REAL,\n",
    "    vader_article_titles_neu REAL,\n",
    "    vader_article_titles_pos REAL,\n",
    "    vader_article_titles_compound REAL,\n",
    "    vader_article_summaries_neg REAL,\n",
    "    vader_article_summaries_neu REAL,\n",
    "    vader_article_summaries_pos REAL,\n",
    "    vader_article_summaries_compound REAL,\n",
    "    vader_article_main_neg REAL,\n",
    "    vader_article_main_neu REAL,\n",
    "    vader_article_main_pos REAL,\n",
    "    vader_article_main_compound REAL,\n",
    "\n",
    "    distilbert_tweet_texts_negative REAL,\n",
    "    distilbert_tweet_texts_positive REAL,\n",
    "    distilbert_article_titles_negative REAL,\n",
    "    distilbert_article_titles_positive REAL,\n",
    "    distilbert_article_summaries_negative REAL,\n",
    "    distilbert_article_summaries_positive REAL,\n",
    "    distilbert_article_main_negative REAL,\n",
    "    distilbert_article_main_positive REAL,\n",
    "\n",
    "    roberta_tweet_texts_negative REAL,\n",
    "    roberta_tweet_texts_positive REAL,\n",
    "    roberta_tweet_texts_neutral REAL,\n",
    "    roberta_article_titles_negative REAL,\n",
    "    roberta_article_titles_positive REAL,\n",
    "    roberta_article_titles_neutral REAL,\n",
    "    roberta_article_summaries_negative REAL,\n",
    "    roberta_article_summaries_positive REAL,\n",
    "    roberta_article_summaries_neutral REAL,\n",
    "    roberta_article_main_negative REAL,\n",
    "    roberta_article_main_positive REAL,\n",
    "    roberta_article_main_neutral REAL,\n",
    "\n",
    "    siebert_tweet_texts_negative REAL,\n",
    "    siebert_tweet_texts_positive REAL,\n",
    "    siebert_article_titles_negative REAL,\n",
    "    siebert_article_titles_positive REAL,\n",
    "    siebert_article_summaries_negative REAL,\n",
    "    siebert_article_summaries_positive REAL,\n",
    "    siebert_article_main_negative REAL,\n",
    "    siebert_article_main_positive REAL\n",
    ");''')\n",
    "\n",
    "cursor.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentiment_record(entry):\n",
    "    processed_entry = dict()\n",
    "    for key, item in entry:\n",
    "        if 'ids' in key:\n",
    "            processed_entry['id'] = int(item)\n",
    "        else:\n",
    "            if item is None:\n",
    "                continue\n",
    "            elif isinstance(item, dict):\n",
    "                for label, score in item.items():\n",
    "                    processed_entry[key + '_' + label.lower()] = score\n",
    "            else:\n",
    "                raise TypeError(f'Unknown entry: {key}, {item}')\n",
    "    return processed_entry\n",
    "\n",
    "def sentiment_pivot(sentiment_data, return_DataFrame = False):\n",
    "    keys, items = zip(*sentiment_data.items())\n",
    "    items = zip(*items)\n",
    "    data = []\n",
    "    for row in items:\n",
    "        data.append(process_sentiment_record(zip(keys, row)))\n",
    "    if return_DataFrame:\n",
    "        data = pd.DataFrame(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg2.connect(host = 'localhost', database = 'nytpopular') as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        for row in sentiment_pivot(vader_sentiment_data):\n",
    "            updateFieldsinDB(row, 'sentiment', cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg2.connect(host = 'localhost', database = 'nytpopular') as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        for row in sentiment_pivot(distilbert_sentiment_data):\n",
    "            updateFieldsinDB(row, 'sentiment', cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg2.connect(host = 'localhost', database = 'nytpopular') as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        for row in sentiment_pivot(roberta_sentiment_data):\n",
    "            updateFieldsinDB(row, 'sentiment', cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with psycopg2.connect(host = 'localhost', database = 'nytpopular') as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        for row in sentiment_pivot(siebert_sentiment_data):\n",
    "            updateFieldsinDB(row, 'sentiment', cursor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52dc165078b550922a92e20afc6187e50fa6838252eb052b89f3983eda27ca00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
