{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "from gensim.models import Phrases, LdaModel\n",
    "from gensim import corpora\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import logging\n",
    "from itertools import repeat, chain\n",
    "\n",
    "import tables\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "multiprocessing.set_start_method('fork') # Because Mac OS does not default to forking processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bag of words\n",
    "\n",
    "Load words from SQL database, construct bigrams/trigrams, build Gensim dictionary, build Gensim bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load bag of words for just the training data\n",
    "\n",
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute('''SELECT wordbags.id, wordbags.bag\n",
    "                        FROM wordbags\n",
    "                        LEFT JOIN traintest\n",
    "                        ON wordbags.id = traintest.id\n",
    "                        WHERE traintest.split = 'train';''')\n",
    "    article_bow = cursor.fetchall()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Phrases((doc for _, doc in article_bow), min_count = 20) # Default threshold is 10\n",
    "trigrams = Phrases(bigrams[(doc for _, doc in article_bow)], min_count = 50) # And some 4-grams\n",
    "\n",
    "bigrams.save('GensimModels/train_bigrams')\n",
    "trigrams.save('GensimModels/train_trigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May be memory-intensive depending on corpus size.\n",
    "# Would be better to stream it into a file with smart_open\n",
    "\n",
    "grams_list = list(trigrams[(bg for bg in bigrams[(doc for _, doc in article_bow)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(grams_list)\n",
    "dictionary.filter_extremes(no_below = 20, no_above = 0.5)\n",
    "dictionary.save('GensimModels/train_dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in grams_list]\n",
    "corpora.MmCorpus.serialize('GensimModels/train_corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LDA model\n",
    "\n",
    "Gensim uses a variational Bayes approach described in Hoffman *et al*. Online Learning for Latent Dirichlet Allocation, *NIPS* (2010). The posterior distribution is approximated by a trial distribution, and ELBO is maximized with respect to variational parameters using coordinate ascent.\n",
    "\n",
    "The variational parameters are:\n",
    "- $\\mathbf{\\phi}$, parameters for multinomial distributions for topics per word\n",
    "- $\\mathbf{\\gamma}$, parameters for Dirichlet distributions for the topic distribution per document\n",
    "- $\\mathbf{\\lambda}$, parameters for Dirichlet distributions for the word distribution per topic\n",
    "\n",
    "For reasons I don't understand, the coordinate ascent is split intoin two steps that are analogous to the EM algorithm. In the E-step, $\\mathbf{\\gamma}$ and $\\mathbf{\\phi}$ are iteratively updated with $\\mathbf{\\lambda}$ fixed. In the M-step, $\\mathbf{\\lambda}$ is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[0] # Weirdly, this is necessary or else id2word will be empty\n",
    "id2word = dictionary.id2token # Mapping from indexes to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_arguments = dict(\n",
    "    corpus = corpus,\n",
    "    id2word = id2word,\n",
    "    chunksize = 2000,\n",
    "    passes = 50,        # Number of passes through entire corpus\n",
    "    iterations = 500,   # Maximum number of iterations in the E-step\n",
    "    eval_every = None,  # Don't compute perplexity\n",
    "    alpha = 'auto',     # Automatically find the Dirichlet prior for the topic distribution per document\n",
    "    eta = 'auto',       # Automatically find the Dirichlet prior for the word distribution per topic\n",
    "    update_every = 1,   # Number of chunks processed in the E-step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model(num_topics, lda_parameters):\n",
    "    logger = logging.getLogger()\n",
    "    loghandle = logging.FileHandler(f'GensimModels/logs/lda_nTopics={num_topics}.log')\n",
    "    logformat = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    loghandle.setFormatter(logformat)\n",
    "    if len(logger.handlers) > 0:\n",
    "        logger.removeHandler(logger.handlers[0])\n",
    "    logger.addHandler(loghandle)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    params = {'num_topics' : num_topics, **lda_parameters}\n",
    "    model = LdaModel(**params)\n",
    "    model.save(f'GensimModels/lda_model_{num_topics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTopics = chain(range(5,30,5),range(30, 200 + 1, 10))\n",
    "\n",
    "with Pool(4) as pool:\n",
    "    pool.starmap(train_lda_model, zip(nTopics, repeat(lda_arguments)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate topic vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTopics = chain(range(5,30,5),range(30, 200 + 1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = Phrases.load('GensimModels/train_bigrams')\n",
    "trigrams = Phrases.load('GensimModels/train_trigrams')\n",
    "dictionary = Dictionary.load('GensimModels/train_dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute('''SELECT wordbags.id, wordbags.bag, traintest.split\n",
    "                        FROM wordbags\n",
    "                        LEFT JOIN traintest\n",
    "                        ON wordbags.id = traintest.id;''')\n",
    "    article_bow = cursor.fetchall()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_topics in nTopics:\n",
    "    class TopicVector(tables.IsDescription):\n",
    "        id = tables.Int64Col()\n",
    "        traintest = tables.StringCol(5)\n",
    "        topicvector = tables.Float32Col(shape=(num_topics))\n",
    "    model = LdaModel.load(f'GensimModels/lda_model_{num_topics}')\n",
    "    with tables.open_file('GensimModels/article_data.h5', mode = 'a') as f:\n",
    "        try:\n",
    "            group = f.root.topic\n",
    "        except tables.NoSuchNodeError:\n",
    "            group = f.create_group('/', 'topic')\n",
    "        table = f.create_table(group, f'ntopics{num_topics}', TopicVector)\n",
    "        entry = table.row\n",
    "        for article in article_bow:\n",
    "            entry['id'] = article[0]\n",
    "            entry['traintest'] = article[2]\n",
    "            entry['topicvector'] = np.array(model.get_document_topics(dictionary.doc2bow(trigrams[bigrams[article[1]]]), minimum_probability = -1))[:, 1]\n",
    "            entry.append()\n",
    "        # table.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric Beta\n",
    "\n",
    "Intuitively, a symmetric beta (eta in Gensim) makes more sense, so trying that here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything below is code duplication of ealier sections of this notebook. It would be better to refactor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTopics = chain(range(5,30,5),range(30, 200 + 1, 10))\n",
    "\n",
    "bigrams = Phrases.load('GensimModels/train_bigrams')\n",
    "trigrams = Phrases.load('GensimModels/train_trigrams')\n",
    "dictionary = Dictionary.load('GensimModels/train_dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary[0] # Weirdly, this is necessary or else id2word will be empty\n",
    "id2word = dictionary.id2token # Mapping from indexes to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpora.MmCorpus('GensimModels/train_corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_arguments = dict(\n",
    "    corpus = corpus,\n",
    "    id2word = id2word,\n",
    "    chunksize = 2000,\n",
    "    passes = 50,        # Number of passes through entire corpus\n",
    "    iterations = 500,   # Maximum number of iterations in the E-step\n",
    "    eval_every = None,  # Don't compute perplexity\n",
    "    alpha = 'auto',     # Automatically find the Dirichlet prior for the topic distribution per document\n",
    "    eta = 'symmetric',  # SYMMETRIC BETA\n",
    "    update_every = 1,   # Number of chunks processed in the E-step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model_symmetricbeta(num_topics, lda_parameters):\n",
    "    logger = logging.getLogger()\n",
    "    loghandle = logging.FileHandler(f'GensimModels/logs/lda_nTopics={num_topics}_symmetricbeta.log')\n",
    "    logformat = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    loghandle.setFormatter(logformat)\n",
    "    if len(logger.handlers) > 0:\n",
    "        logger.removeHandler(logger.handlers[0])\n",
    "    logger.addHandler(loghandle)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    params = {'num_topics' : num_topics, **lda_parameters}\n",
    "    model = LdaModel(**params)\n",
    "    model.save(f'GensimModels/lda_model_{num_topics}_symmetricbeta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(4) as pool:\n",
    "    pool.starmap(train_lda_model_symmetricbeta, zip(nTopics, repeat(lda_arguments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = 'localhost', database = 'nytpopular')\n",
    "with conn.cursor() as cursor:\n",
    "    cursor.execute('''SELECT wordbags.id, wordbags.bag, traintest.split\n",
    "                        FROM wordbags\n",
    "                        LEFT JOIN traintest\n",
    "                        ON wordbags.id = traintest.id;''')\n",
    "    article_bow = cursor.fetchall()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_topics in nTopics:\n",
    "    class TopicVector(tables.IsDescription):\n",
    "        id = tables.Int64Col()\n",
    "        traintest = tables.StringCol(5)\n",
    "        topicvector = tables.Float32Col(shape=(num_topics))\n",
    "    model = LdaModel.load(f'GensimModels/lda_model_{num_topics}_symmetricbeta')\n",
    "    with tables.open_file('GensimModels/article_data_symmetricbeta.h5', mode = 'a') as f:\n",
    "        try:\n",
    "            group = f.root.topic\n",
    "        except tables.NoSuchNodeError:\n",
    "            group = f.create_group('/', 'topic')\n",
    "        table = f.create_table(group, f'ntopics{num_topics}', TopicVector)\n",
    "        entry = table.row\n",
    "        for article in article_bow:\n",
    "            entry['id'] = article[0]\n",
    "            entry['traintest'] = article[2]\n",
    "            entry['topicvector'] = np.array(model.get_document_topics(dictionary.doc2bow(trigrams[bigrams[article[1]]]), minimum_probability = -1))[:, 1]\n",
    "            entry.append()\n",
    "        table.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52dc165078b550922a92e20afc6187e50fa6838252eb052b89f3983eda27ca00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
